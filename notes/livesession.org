#+STARTUP: indent
#+STARTUP: overview
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
* TODO Getting started                                            :CLASSROOM:

** Basic Information

- Instructor :: Piyush <ppk@.>
- TAs

  1. Lijo M Jose
  2. Julin Shaji

- Repository :: https://gitlab.com/piyush-kurur/popl

Hosting site for git repositories

- Moodle website :: https://lms.iitpkd.ac.in
- Youtube channel :: https://www.youtube.com/channel/UCfCBIg1UZAYs5rjCR9GiBow
- Google meet link :: Link already sent to you

** Format of the lectures

/Flipped classroom/ model of lectures

- There will be weekly videos that I will post at the Youtube channel
- Interactive session weekly once (Monday 10:00-11:00 hrs
- Lab Interactive session (PoPL Lab) (Monday 14:00-midnight hrs)
- TAs will have a surprise interaction with you to check you are not asleep

** Grading

- Try have a lot of Short quizes (Consult the quiz schedule). Over
  moodle (best n of n+1).  (60-80%)

- End sem

** TODO At Home (Homework 0)

1. All of you have a laptop/desktop with internet

2. Should have a working GNU/Linux partition (Debian/Ubuntu)

3. Basics of computing

4. Start using a good editor (Emacs)

5. Should know how to install some software.

#+BEGIN_SRC bash

sudo apt install git   # debian/ubuntu
sudo apt install mlton

#+END_SRC

** Things to learn in the process of doing this course

1. git
2. make files etc dev tools on linux

** What this course is about

- Basic principles by which programming languages are designed

  - Functional Programming (SML - Standard Meta Language)
  - Rust (Imperative programming language)
  - Logical programming (Prolog)
  - Type systems, Concurrency, Modularity - Principles behind programming
languages.

- Compilers in the next semester

- Lab component

  - Using the theory and putting it into practice.
  - Side benefits (git, general programming practices)
  - Compilers course next semester some of these things will put into practice


* Functional Programming
SCHEDULED: <2021-08-23 Mon>

**  Why SML?

Functional programming through Standard ML (SML)

- ML :: means meta-language.

- History is the implementation of a LCF proof assistant.

  LCF  is the object language and ML was  designed to be the
  meta language for LCF.

- SML :: is probably the only programming language which has a /formal
  semantics/ (meaning).

  1. Syntax :: Grammar of the language.

  2. Semantics :: Meaning of the language. There is a formal semantics
     of SML that has been checked through a proof assistant.

     SML is "type safe"

  3. LCF system's correctness depends on SML's type safety and hence type safety of
     SML was very important.

     For each of the theorems you create a "proof object" and if SML was type unsafe you
     can create "wrong proofs".

  4. As a language it is really simple but some deep concepts are
     available. SML has a concurrency mode (Concurrent ML which is
     really advanced).

  5. Introduces to a lot of programming language concepts.

     1. Type systems
     2. Continuations CPS system
     3. Module systems.

#+begin_src C

  int main()
  { int c=42;
    char *hello = "hello";
    double x y ;

    double av = 1/2 * (x + y); // ill typed expression
    printf("%s %d %p", c , hello, c + hello);
    hello[c]= 0;
    return 1;
  }
#+end_src

- Type safety :: If the compiler says that your program has not "type mismatch", there
should not be a illegal instruction executed during the run of the program.

- Type Unsafe languages :: There are programs where the compiler will not flag a "type mismatch"
  but will lead to a "Stuck" state.



Accessing ~hello[i]~ can result in undefined behaviour.






** Two SML compilers/interpreters.

*** Two phases of development of an application

- Development phase ::

  + What is important ?

    1. Speed of compilation
    2. Quick feedback
    3. Quickly checking out a piece of code (prototyping)
    4. Better error messages when things are not okey.

  + What you do not care about

    1. Speed of the resulting executable
    2. Space taken by the resulting code.
    3. You do not care about efficiency of execution.


- Deployment phase ::

  + What you care about is efficiency of the executable and not the feedback to the developer.


*** SML/NJ - New Jersey SML interpreter

smlnj should be used in the development phase

smlnj can be used to build a standalone executable but it is not
recommended.

*** Mlton - Compiler for SML

mlton compiler should be used in the production phase.
mlton is a compiler and not an interpreter.
* Basics of Standard ML                                           :CLASSROOM:
SCHEDULED: <2020-08-10 Mon>

* Programming model for SML
SCHEDULED: <2021-08-30 Mon>

There are two views to the programming language.

1. The view of the programmer ::

   The things of interest are the high level info of what the given
   program does.

   The programmer needs to think of how to accompish the programming
   task at hand using high level programming concepts.

   1. How do I break the big task into smaller task ?

   2. Can I generalise certain programming patterns and refactor them into
      functions ?

   3. Can I use already existing library ?

   4. What should I do so that I make less errors ?

   5. Is the following code understandable to others ?

2. The view of the implementer.

   The things of interest are those that guide efficient implementation
   of the "programmer"s view.

   1. Is it possible to vectorise certain code ?

      a = a + c
      ap = ap + cp

      [a, ap] = [a , ap] + [c,cp]

   2. Can a certain recursive code be converted to loops ?

   3. Which variables need to be in registers at what point of time ?


3. If you are working on Computer Arch, then the assembly language itself
   is a "programmers view" ?



** The programming model for Languages

- It is a mental model of what the program /is/ and what it /does/.

- It might not be the actual implementation model which is typically
  the machine model.

- The distinction between the programming model and the implementation
  model is important because the programming model is often
  simpler/convenient for Humans.

- Moral :: Programming Languages are both for Humans to understand and
  for machines to perform.

*** C Language

- Variables that are memory cells
- assignments that update memory cells
- There is a control flow which determines in what order the instructions are executed

** Programming model for SML

*** Variable Bindings

- Variables :: x y z etc
- Expressions/terms :: 10 , 5 + 2, can be a variable
- Values :: are expressions that are in their "simplest" form.
- Environment :: is a function from Variables to values.

When an expression is a value ?

Values ⊆ Expressions

The programming model for sml is the following :


1. The program consists of a sequence of /variable bindings/.

2. A variable binding looks like ~val <variable> = <expr>~

3. A subset of expressions are called /Values/. These are
   those expressions that are in the simplest form.

4. The SML language when it encounters a binding ~val x = e~,
   evaluates (tries to) the Right Hand Side e of the binding (which is
   an expression) to its simplified form (which is a value) ~v~. And
   then it updates the /environment/ by binding the variable to ~x~ to
   ~v~. At this point of "time" we say that the variable x is /bound/
   to the value v.





#+BEGIN_SRC sml

   (* Environment is stdlib + {}*)
   val x = 10     (* Binds the variable x to the value 10 *)

   (* Environment becomes stdlib + {x = 10} *)

   val y = 5 + 2  (* RHS = 5 + 2 (expression) ----> 7 (value)
                       Bind the value 7 to y
                  ,*)
    (* Env = stlib + { x =10 ; y = 7 }  *)


   val z = x + y  (* RHS is not value.
                      x + y ----> 10 + y (substitution of x)
                            ----> 10 + 7 (substitution of y)
                            ----> 17
                      Binds z to 17
                      ,*)

  (* Env = stdlib + {x = 10 ; y = 7; z = 17 *)

   val amitabbachan = "Big B"

   val z1 = amitabbachan
   (* Env = ? *)

#+END_SRC

#+RESULTS:
: val x = 10 : int
: val y = 7 : int
: val z = 17 : int
: val amitabbachan = "Big B" : string
: val z1 = "Big B" : string

*** Expressions

- Values are certain expressions which are no more reducible.
- Values are expressions that are in reduced form (simplified form)
- Values are defined based on the context


**** Examples

- 2     (value)
- 3 + 5 (not value) where as 8 is value
- x     (not value) but we need to figure it out from the binds applicable at the point

- "hello"



*** Computations/effects happen while expressions are evaluated.
** Hello world

#+BEGIN_SRC sml

val x = print "Hello World\n"

#+END_SRC

#+RESULTS:
: Hello World
: val x = () : unit

** Side effects and pure expressions.

#+BEGIN_SRC sml

val x = print "hello\n"  (* Prints the string "hello" as a side effect *)
val y = ()
val z = x (* No side effect as () is a value and
             hence need no reduction *)

val z = () (* This is equivalent to the above binding *)

(*

 print "hello\n" (expression)
   ----> ()

In the process there is a side effect of
printing the "hello\n"

*)

#+END_SRC

#+RESULTS:
: hello
: val x = () : unit

Certain expressions on reduction has side effects
e.g = ~print "hello\n"~ . Evaluating it has an effect on
the outside world.


Those Expressions whose evaluation  do not have side effect is
called a /pure expression/

1. ~print "hello"~ is an expression that is /not/ a value
2. It reduces to the value ~()~
3. The reduction has the side effect of printing "hello"
4. However, the resulting value ~()~ has no side effect.


An expression ~e~ which reduces to a value ~v~ is said to be /pure/
if every binding of the form ~val x = e~ can be replaced by the binding
~val x = v~ without changing the behaviour of the program.

Any expression that does not have the above property is said to have
"side-effects".

*** SML is /not/ a pure functional programming language

Because there are expressions (e.g ~print "hello"~) whose reductions
have side effect.




** Unit

1. Type called ~unit~
2. It has only one value ~()~ (call this unit, void)

#+BEGIN_SRC sml
fun identity x  = x
fun hello ()   = print "hello\n"
fun hello1 x   = print "hello1\n"
val z          = hello () (* --> (1) *)
val u = identity 10
val v = identity "hello"
val u1 = hello1 42        (* --> (2) *)
val u2 = hello1 "hello"   (* --> (3) *)
val _  = hello ()
val _  = print "This is just printing without binding\n"

#+END_SRC

#+RESULTS:
#+begin_example
hello
hello1
hello1
hello
This is just printing without binding
val identity = fn : 'a -> 'a
val hello = fn : unit -> unit
val hello1 = fn : 'a -> unit
val z = () : unit
val u = 10 : int
val v = "hello" : string
val u1 = () : unit
val u2 = () : unit
#+end_example





* Polymorphism (Parametric)

** Recap

#+begin_src sml
  val x : int = 10     (* binds x to 10 *)
  fun f x = x + 1	   (* can also be recusive,
                              binds f to the function that takes x and returns x +1 *)

  fun g (x : int) = x + 1
#+end_src

- Strong typing :: Every expression in SML has a type and the compiler
  enforces type checking at compile time.

- Type inference :: When ever an expression can be given a valid type, the compiler
  infers the type (it infers the /most general type/).

- Parametric polymorphism :: A function can work with different types.
  poly-morphism morphism is another word for function and poly morphism means multiple
  possible functions.

  You can think of ident as a /family/ of functions one for each possible instantiation of
  'a with types.

  Note that SML does type checking at compile time unlike say Python. But the combination of
  Type inference + Parametric polymorphism make it look like you do not have to worry about
  declaring the types.

 - Adhoc polymorphism or Overloading ::  C++ without template. Java without generics
   provide a way to give same name to different functions. The actual function that is
   used /depends on the type/

   E.g overloading of + or << in C++   << is bitwise left shift as well as cout << "hello"

   (1) << takes two ints then it should be the left shift of Int
   (2) if it is a ostream and the right operator is string then it is a printing function

  - SML does not have overloading but there are other ways for it.

#+begin_src sml
  fun ident x = x   (* int -> int, or string -> string,  (int -> string) -> (int -> string) *)
  fun idInt (x : int) = x
  fun curry f x y = f (x,y)
#+end_src

SML infers the type ~'a -> 'a~ here ~'a~ is a type variable


What type really are ?

1. Types are specifications to your value.

2. As a client hiring you for writing code, I have to give you my requirements (or Specs)
   It is your job to write the code for it.

   'a -> 'a

- Moral of the story :: whenever possible write the most general function.

Map function. It takes a function f and a list l and gives the list
obtained by applying f on every element of the list.


~map : ('a -> 'b) -> 'a list -> 'b list~

Let g be any function that has the above polymorphic type. g has to
apply f on the list, drop or duplicate the elements of the list, and
rearrange in some specific way












* Pattern matching

#+begin_src sml
   val x = 10      (* an equantion that says in this context x is 10 *)
   fun f x = x+1   (* we say f is that function that satisfies f x = x + 1 *)
   fun konst _ = 10
   fun first (x, _) = x (* I have used variable pattern, wild card and tuple pattern *)

   fun foo ( _ , _ , y) = y + 1


   (*
      fun foo p = ... where p is the pattern (p₁, p₂)  p: 'a * b where p₁ : 'a, p₂ : 'b
                            p₁ is the pattern (p₁₁, p₁₂)  'a = 'a₁ * 'a₂
                            p₁₁ and p₁₂ are both _

                            p₂ is the pattern y : 'b , 'b has to be int because RHS has y + 1
                              RHS type is 'c so 'c and 'b has to be type int

     foo : ('a₁ * 'a₂) * int -> int

     foo : A * B -> C  where A = 'a₁ * 'a₂

     foo : A * B * C -> D
  ,*)

   fun bar (   _   , y) = y + 1

   (* length : 'a list -> int *)
   (* fun length (xs : 'a list) = ... : int *)
   fun length []        = 0
     | length (_ :: ps) = 1 + length ps

   (* map f [x₁,....,xₙ] = [ f x₁ , f x₂ , .... , f xₙ ]

      map : 'a ->           'b           -> 'c
      map : ('a₁ -> 'a₂) -> 'b₁ list     ->'c₁ list
      map : ('a₁ -> 'a₂) -> 'a₁ list     -> 'a₂ list
      map : ('a -> 'b) -> 'a list -> 'b list

    ,*)
   (* fun map f xs = ....  *)
   fun map f (p :: ps) = f p :: map f ps
     | map _ []        = []

#+end_src

#+RESULTS:
: val x = 10 : int
: val f = fn : int -> int
: val konst = fn : 'a -> int
: val first = fn : 'a * 'b -> 'a
: val foo = fn : 'a * 'b * int -> int
: val bar = fn : 'a * int -> int

Patterns

We define what are patterns. A pattern on the LHS servers two purposes In an equation
like ~fun f p = e~ , p restrict the allowed values to certain forms and also leads
to variable binding. We call this process "pattern matching"




1. A variable :: A variable pattern ~x~ matches any value (of appropriate type) and binds
   x to it for use in RHS

2. A wild card  i.e ~_~.  The wild card patter ~_~ matches any value but that value is not
   available as binding on the RHS. One uses a wild card when one does not care about the value.

** Tuples

What is the type ~A * B~ ? To answer that question I have to give you
what are the elements of the type ~A* B~. Or in other words what are
the values ~v~ such that ~v : A * B~ ?

- Construct :: Let v₁ : A and v₂ : B be values of type A and B respectively then
   (v₁ , v₂) is a value and is of type ~A * B~

- Take apart ::  If ~v : A * B~ is a value then there is ~v₁ : A~ and ~v₂ : B~  such that
   ~v = (v₁ , v₂)~

   Which means if you want to write a function f: A*B -> C then what you need to specify
   what the function value is on a tuple (v₁, v₂) where v₁ : A and v₂ : B.


*** Tuple Pattern

If p₁ and p₂ are patterns then (p₁ , p₂) is also a pattern.

When you use the tuple pattern : The match is successful if and only if the value
is of the form (v₁, v₂) where p₁ matches v₁ and p₂ matches v₂

*** Generalising to n-tuples for any n ∈ ℕ

If A₁ .... Aₙ are types then the type A₁ * ... * Aₙ consists of values

- Construction :: If v₁ : A₁ .... vₙ : Aₙ then (v₁,....vₙ) : A₁ * ... * Aₙ
- Taking apart :: The only possible values of the n-tuple type A₁ * ... * Aₙ
  is the one obtained by applying the construction rule

  The pattern for n-tuple would be

  (p₁,...,pₙ) where pᵢ is a pattern of type Aᵢ


** Lists

What is a ~'a list~ ? Again I need to tell you what the elements of this type are.


- Construction ::

  1. The empty list ~[]~ is a /value/ of type ~'a list~.
  2. If ~v : 'a~ and ~vs : 'a list~ then ~v :: vs : 'a list~ where ~v~ and ~vs~ are values

- Taking apart :: The only possible values of type ~'a list~ are the ones created out of
  the above two rules.

This types that are defined by a set of finite rules together with the appropriate Taking apart
condition

The taking apart condition is the rule that says that the type is the "smallest" type with
the Construction rules.

*** List patterns.

1. [] is a list pattern that matches an empty list of type 'a list

2. If ~p~ and ~ps~ are patterns of types ~'a~ and ~'a list~ respectively then
   ~p :: ps~ is a pattern of type ~'a list~.

   p :: ps  matches a list v :: vs where p match v and ps matches vs

   The bindings created are the bindings that is the result of matching p to v
   and ps to vs.

   #+begin_example sml
     fun g x = x + 1
     fun f (g y) = y - 1 (* Not Allowed *)

                      (* Check if the value given to f as argument looks like g y for some y *)
		            (* Then bind y to that value *)


   #+end_example

* Algebraic data types and pattern matching
SCHEDULED: <2021-09-20 Mon>

** Recap

We briefly discussed pattern matching and how to use it to define
functions.

#+begin_example sml
  val x     = expr
  fun f x y = exp

  (* The general form of bindings *)
  val pat = expr  (*
                    1. expr is reduced to a value  say v
                    2. v is matched with the pattern pat
                    3. If the match failed it is a runtime error
                       (typically due to non-exhaustive pattern matching )
                    4. If the pattern pat matches with v the associated bindings
                       are done at the top level.
                       *)

  fun f p₁₁ ... p₁ₙ = e₁
    | f p₂₁ ... p₂ₙ = e₂
    | ....
	| f pₘ₁ ... pₘₙ = eₘ

  val x = let val pat = e1    (* Only applicable within this let and the
                                 associated expression betwen "in" and "end"
                              *)
              fun f p1 = e2
          in
             exp
          end


#+end_example

#+begin_src sml
  val (x,y) = ("hello", 2)
  val _     = print "hello\n"
  val _     = 2
  fun head (x :: xs) = x
  (* val (x,_) = "hello"  (* This is an error because tuple pattern cannot match string *)  *)
  (* val x::xs  = []  (* This is a pattern match failure that manifest itself at run time *) *)
  val x :: xs = [1,2]

#+end_src

#+RESULTS:
#+begin_example
stdIn:28.5-28.20 Warning: binding not exhaustive
          x :: xs = ...
stdIn:25.5-25.23 Warning: match nonexhaustive
          x :: xs => ...

hello
val x = <hidden-value> : string
val y = 2 : int
val head = fn : 'a list -> 'a
val x = 1 : int
val xs = [2] : int list
#+end_example






** Basics of Algebraic types.

#+begin_example sml

  datatype ('a, 'b, 'c) Name = C1
                             | C2 of <τ>
                             (* where <τ> is some type in SML that may involve the type variables 'a, 'b, 'c etc *)

#+end_example

The result of the above definition is

1. A new type ~('a, 'b, 'c) Name~ is born

2. Constructors are new functions ~C1 : ('a,'b,'c) Name~
   and ~C2 : τ -> ('a,'b,'c) Name~



*** What are the values of type ('a,'b,'c) Name ?

Recall values are subset of expressions that are in reduced form (SML
does not reduce it any further. e.g 1 is a value where as 1 + 1 is not
value.



1. ~C1~ is a value of type ('a,'b,'c) Name
2. If ~v~ is a value of type ~τ~ then  ~C2 v~ is a value of type
   ~('a,'b,'c ) Name~

   #+begin_src sml
     datatype Foo = Foo1
                  | Foo2 of int

			  (* Here ~Foo2 1~ is a value but Foo2 (1 + 1) is not value *)
   #+end_src

3. Suppose we have an expression of the kind ~C2 e~ then SML tries to reduce ~e~ to value
   ~v~ and suppose it succeeds then the value to which ~C2 e~ reduces is ~C2 v~


*** What are the patterns of type ('a,'b,'c) Name ?

There are two possible patterns of type ('a, 'b, 'c) Name

1. ~C1~ is a pattern of type ~('a, 'b, 'c) Name~ and it matches only
   the value ~C1 : ('a,'b,'c) Name~.  No bindings are created as a result.

2. ~C2 p~ is a pattern of type ~('a,'b,'c) Name~ for all pattern ~p :
   τ~.  It matches all values of the kind ~C2 v~ where ~p~ matches
   ~v~. The resulting bindings are the bindings created out of the
   match of ~p~ with ~v~.


#+begin_src sml

  (* binary tree with nodes decorated by elements of type 'a *)
    datatype 'a Tree = Empty
                     | Node of 'a Tree * 'a * 'a Tree

                                (* inorder :  'a Tree -> 'a list  *)
    (* fun inorder t = ... *)
    fun inorder Empty             = ...
      | inorder (Node (lt,a,rt) ) = ...

    (* depth : 'a Tree -> int *)
    (* fun depth t = ... *)
    fun depth  Empty              = 0
      | depth (Node (lt, _, rt))  = let val dl = depth lt
                                        val dr = depth rt
                                    in  if dl > dr then dl + 1
                                        else dr + 1

#+end_src

#+RESULTS:
: datatype 'a Tree = Empty | Node of 'a Tree * 'a * 'a Tree


** Algebraic data types in action

We want to design a calculator.

You capture the AST (abstract syntax tree/parse tree) of your language
(in this case expressions) as an algebraic data type.


#+begin_src sml

  datatype Expr = Const of int
                | Plus  of Expr * Expr
                | Mul   of Expr * Expr

  fun eval (Const x)        = x
    | eval (Plus (e1, e2))  = e1 + e2
    | eval (Mul  (e1, e2))  = e1 * e2

  val parse : string -> Expr option (* this converts user input given as
                                       string to expression To actually
                                       run your calculated you need
                                       parse but the essence of the
                                       calculator program is present in
                                       the type Exp and the function
                                       eval.
                                     *)

#+end_src

#+begin_src sml

datatype 'a option = SOME of 'a | NONE

#+end_src


* Modular programming.
SCHEDULED: <2021-09-27 Mon>

Modular programming means break down the program into smaller
components which are separtely programmed, tested and linked together.

1. We want to control namespace

   #+begin_src sml
     val x = 10
   #+end_src

   i.  I want to be able to use the same names for different (but
       often similar things)

       In SML it is done by wrapping things inside a structure.

       #+begin_src sml
         structure A = struct
            val x = 10
         end (* end of structure A *)

         structure B = struct
            val x = "hello"
         end (* end of structure B *)

         val z1 = A.x
         val z2 = B.x
       #+end_src

       #+RESULTS:
       : structure A : sig val x : int end
       : structure B : sig val x : string end
       : val z1 = 10 : int
       : val z2 = "hello" : string

   ii. I should be able to say that a name (for example ~x~) in a
       context it means some particular ~x~ that has been defined.

       #+begin_src sml
         structure A = struct
            val x = 10
         end (* end of structure A *)

         structure B = struct
            val x = "hello"
         end (* end of structure B *)

         (* There is no x here only A.x and B.x *)

         open B
         val z1 = let open A
                     (*
                      Local bindings that are only valid
                        inside the expression between "in" and "end"
                          let
                               <val or fun bindings>
                          in

                            end
                      ,*)
                 in x + x  (* x comes from A *)
                 end

          val z2 = x (* This comes from B *)
          val z3 = A.

       #+end_src

       #+RESULTS:
       : structure A : sig val x : int end
       : structure B : sig val x : string end
       : opening B
       :   val x : string
       : val z1 = 20 : int
       : val z2 = "hello" : string
       : val z3 = 10 : int

   iii. Abstraction over modular structures. I should be able to build
        newer modular components based on other modular components.

        Point iii. is unique to SML (ocaml, Coq ....)

        For this SML introduces two important constructs namely signatures and functors.



**  Signatures

        #+begin_src sml

          structure A :
                    sig (* Hides y because it is not present in the signatures *)
                        val x : int
                    end
          = struct
            val x = 10
            val y = 100
          end


        #+end_src

        #+RESULTS:
        : structure A : sig val x : int end

        There are reasons to make sure that certain functions should not be exposed to
        the outside world.

        #+begin_src sml
          structure Counter :
                    sig
                        val incr : unit -> unit
                        val showValue : unit -> int
                    end =
          struct
               val ctrRef = ref 0
                     (* Counter implementation *)

               fun incr () = ()      (* silly increment not correct *)
               fun showValue () = 42 (* silly show value that always gives the answer *)
          end
        #+end_src

        #+RESULTS:
        : structure Counter :
        :   sig
        :     val incr : unit -> unit
        :     val showValue : unit -> int
        :   end


        Implementation of Binary search tree. Assumes that the "BST"
        property is true on all internal nodes. If you allow the user
        to "see" the actual type, they could potentially violate the
        property. So you would want to hide

        1. Functions, values etc

        2. And also types.


        #+begin_src sml
          structure A :  (* Transparent signature *)
                    sig type t
                        val pub : t
                    end
          = struct
               type t = int
               val pub = 42
               val prv = 100
          end
          val z = A.pub + A.pub

          structure B :>  (* Opaque/translucent signature *)
                    sig type t
                        type t1 = int
                        val pub : t
                            val pub1 : t1
                    end
          = struct
               type t = int
               type t1 = int
               val pub  = 42

               val pub1 = pub + pub
               val prv = 100
          end
          (* val z1 = B.pub + B.pub (* error *) *)
          val z11 = B.pub1 + B.pub1 (* not
                             an error *)
          val z2 = B.pub

        #+end_src

        #+RESULTS:
        #+begin_example
        structure A :
          sig
            type t = int
            val pub : t
          end
        val z = 84 : int
        structure B :
          sig
            type t
            type t1 = int
            val pub : t
            val pub1 : t1
          end
        val z11 = 168 : int
        val z2 = - : B.t
        #+end_example

        - NOTICE: In the result, A's signature mentions that A.t is
          int whereas B's signature does not mention it The reason for
          this difference is because of ":" vs ":>"



        Clearly by using the translucent signature (i.e using :>
        instead of :) scheme we can hide the data type behind BST and
        allow the creation of BST only through controlled interface.

** Functors

#+begin_src sml

   datatype ordering = LT | EQ | GT

   signature KEY = sig
     type key
     val compare : key -> key -> ordering
   end

   datatype 'a Tree = NullTree
                    | Node of 'a Tree * 'a * 'a Tree

   signature DICT = sig
     structure Key : KEY
     type key = Key.key
     type 'value t   (* this is your dictionary type
                            polymorphic in value that it stores against
                            key of type Key.key
                          ,*)
     val empty : 'a t
     val single : key -> 'a -> 'a t
     (* val insert : Key.key -> 'a -> 'a t -> 'a t *)
   end

   (* BST functor takes a structure of signature KEY and gives a structure of signature
      DICT.

     structure BST  :> DICT =
       struct

       end

      BST is a "function" that takes a structure K of signature KEY and "returns"
      a structure of signature DICT.

      compare it with something like this.
      fun foo (x : int) : int = x + 42

   ,*)
   functor BST ( K : KEY ) :> DICT
   = struct
        structure Key = K
        type key = Key.key

        (*
        type 'a t     =  (key * 'a) Tree  (* binary tree with nodes
                                                 decorated by elements of type key * 'a
	                                      *)

        ,*)
        datatype 'a t = Null
                      | Node of 'a t * (K.key *  'a) * 'a t

        val empty = Null
        fun single k a = Node (empty, k , a , empty)
   end

   functor ListDict ( K : Key) :> DICT
   = struct
        structure Key = K
        type key K.key
        type 'a t = (key, 'a) list
        val empty = []
        val single k a = [ (k,a) ]
   end

   structure IntKey : KEY = struct
      type key = int
      fun compare x y = if x < y then LT
                        else if x = y then EQ
                        else GT
   end

   structure IntDict = BST (IntKey)

                         (* IntDict is a polymorphic (in value)
                            dictionary data structure where the key is int
                             ,*)

   fun foo (x : int) : int = x + x
  val x = IntDict.empty

  (* val single : Key.key -> 'a -> 'a t  *)

  (* val y = IntDict.single 42 (* "The answer to Life, Universe and everything" *) *)


#+end_src

#+RESULTS:
#+begin_example
datatype ordering = EQ | GT | LT
signature KEY =
  sig
    type key
    val compare : key -> key -> ordering
  end
datatype 'a Tree = Node of 'a Tree * 'a * 'a Tree | NullTree
signature DICT =
  sig
    structure Key :
      sig
        type key
        val compare : key -> key -> ordering
      end
    type key = Key.key
    type 'a t
    val empty : 'a t
    val single : key -> 'a -> 'a t
  end
functor BST(K: sig
                 type key
                 val compare : key -> key -> ordering
               end) :
           sig
             structure Key : <sig>
             type key = Key.key
             type 'a t
             val empty : 'a t
             val single : key -> 'a -> 'a t
           end
structure IntKey : KEY
structure IntDict : DICT
val foo = fn : int -> int
val x = - : 'a IntDict.t
#+end_example


Analogy between modular programming and typed functional programming

| Modular programming     | Typed functional programming   |
|-------------------------+--------------------------------|
| structures              | values                         |
|-------------------------+--------------------------------|
| signatures              | types                          |
|                         |                                |
| interface of the        | types govern what can be done  |
| structure               | to/by values                   |
|                         |                                |
| specification of the    | specification of the value     |
| structure               |                                |
|-------------------------+--------------------------------|
| functors                | functions                      |
|                         |                                |
| Takes structures        |                                |
| to new structures       |                                |
|                         |                                |
| Interface is controlled | takes values of a specfic type |
| by the spec (signature) | to values of another type      |
| of the said structurres |                                |
|-------------------------+--------------------------------|


There are two levels in which you do functional programming

1. The usual values level

2. At the module level.

3. Both are "typed functional programming language". At the module
   level "values" are structures (modular structures), "types" are
   signatures and "functions" are functors.



** Key point of SML-style modules.


1. Any programming language can be equipped with a module system similar
   to SML. Unfortunately lot of the "modern" languages do not yet have
   anything even close to it.


2. Modules are zero cost abstractions. In fact mlton has a
   "defunctorisation step" where it actually "removes" all structures
   and modules.



#+begin_src C
   /* This is just a product type and has nothing to do with SML style structures (modulues)
   */
    struct A {
       int x;
       int y;
    };

#+end_src

#+begin_src sml
val u = { x = 10, y = 100 }  (* A product type int * int but with names for fields *)

#+end_src

#+RESULTS:
: val u = {x=10,y=100} : {x:int, y:int}


* Application: Atom type and Hash consing.
SCHEDULED: <2021-10-11 Mon>

- Docs for Atom structure ::

  https://www.classes.cs.uchicago.edu/archive/2015/spring/22620-1/atom-sig.html


Let us take strings. What is the time it takes to compare two strings of length n.

- n * comparison of a Byte = n instructions on your processor

- The time taken to compare for example two words in processor  1 instruction.


1. Instead of comparing two strings we compare the corresponding pointers.

   String = Pointer * length

   - Good :: It is fast (1 instruction per comparison)

   - Bad  :: It is incorrect. If two strings are represented by the
     same pointer then they are indeed equal. But there can be strings
     which are same but are saved in different locations of the
     memory.

   - Worse :: If you use pointers to order strings (let us say to put it in
     a BST). The transitivity property of ordering is no more true. It is not
     an "ordering"


2. Structural vs referentially equality/comparison.

   What we want is structural equality/comparison what we get is
   referential equality/comparison


** Question is

Can I do essentially structural comparison at the speed of referential comparison ?
Hash consing is a way to do this.


- Observation :: Suppose you maintain the property that given any two
  strings "of interest" are the same if and only if they are
  referentially same, then referential equality is enough.

- Observation :: If the above property is true then the comparison of
  the references should give an ordering (not necessarily the
  structural ordering, i.e. Dictionary ordering in the case of
  strings) .

**  Atom Structure

Atom.atom is a representation of string which is fast to compare,
i.e. typically 1 instruction.

Application is typically in compilers/symbolic processors where you
want to look up strings often. In the compiler, this usually comes
up in symbol table lookups.

If s₁ and s₂ are two strings such that a₁ = atom s₁ and a₂ = atom s₂ then

1. same (a₁, a₂) if and only if s₁ == s₂ (however it is expected that
   same (a₁, a₂) will be faster


2. compare (a₁, a₂) need not be the same as compare(s₁, s₂). However
   the ordering that one gets via compare (a₁, a₂) should give a total
   ordering.


#+begin_src sml
  open Atom
  val x = atom "hello"
  val y = atom "world"


  datatype var Expr = Const of int
                    | Plus of Expr * Expr
                    | Var of v

  (* Internal expression, i.e. the representation of expression used inside the compiler *)
  type IExpr = Atom.atom Expr

  type UExpr = string Expr (* Expression as seen by the user of the compiler *)

  type 'value Env = 'value AtomMap.map  (* A key value store where the keys are atoms *)

  (* fun eval (env : int Env) (e : IExpr) = ... *)


#+end_src

#+RESULTS:
#+begin_example
[autoloading]
[autoloading done]
opening Atom
  type atom
  val atom : string -> atom
  val atom' : substring -> atom
  val toString : atom -> string
  val same : atom * atom -> bool
  val sameAtom : atom * atom -> bool
  val compare : atom * atom -> order
  val lexCompare : atom * atom -> order
  val hash : atom -> word
val x = - : atom
val y = - : atom
datatype Expr = Const of int | Plus of Expr * Expr | Var of atom
type 'a Env = 'a AtomRedBlackMap.map
#+end_example



** Implementation of Atom

#+begin_src sml
   fun hash_string (x : string) : int = 0  (* A good hash function *)
   signature ATOM = sig
     type atom
     val atom : string -> atom
     val toString : atom -> string
     val compare  : atom * atom -> order
   end

   structure Atom1 :> ATOM = struct
      type atom               = { hashcode : int , symbol : string }
      fun atom x              = { symbol = x , hashcode = hash_string x }

      (* If hash_string is a good hash function then very likely
         the hash values of different strings are different *)


      fun toString (a : atom) = #symbol a
                                      (* Note that toString (atom x) = x *)
      fun compare (a1 : atom , a2 : atom)  =
          case Int.compare  (#hashcode a1 , #hashcode a2) of
               EQUAL => String.compare ( #symbol a1 , #symbol a2)
             | x     => x

                                      (* The comparison is fast  because it is int comparison *)
   end


  (*
    Benign side effect, i.e observationally pure computation *)

   structure Atom2 :> ATOM = struct

      type atom               = { hashcode : int , symbol : string }


      val counter = ref 0    (* We have a counter which will be used to assign hash codes *)
      val ht : atom StringHashTable.hash_table = IntHashTable.mkTable (100, exn)

      fun mkAtom x              = let val cur = !counter
                                      val atm = { symbol = x , hashcode = cur }
                                  in counter := cur + 1;
                                     StringHashTable.insert ht (x, atm);
                                     atm
                                  end

      fun atom x = case StringHashTable.find ht x of
                       NONE => mkAtom x
                    | SOME a => a


      fun toString (a : atom) = #symbol a
                                      (* Note that toString (atom x) = x *)
      fun compare (a1 : atom , a2 : atom)  = Int.compare  (#hashcode a1 , #hashcode a2)

                                      (* The comparison is fast  because it is int comparison *)
   end



#+end_src

#+RESULTS:
#+begin_example
val hash_string = fn : string -> int
signature ATOM =
  sig
    type atom
    val atom : string -> atom
    val toString : atom -> string
    val compare : atom * atom -> order
  end
structure Atom1 : ATOM
structure Atom2 : ATOM
#+end_example


* Lambda calculus

1. We study (untyped) lambda calculus
2. The typed version
3. Typing rules
4. Type inference algorithm.



** Lambda calculus as the simplest functional programming language.


*** Syntax of the language.

1. Variables
2. Apply functions on other values.
3. Some way to create functions.

   The lambda calculus "programs" are actually expressions that are defined
   by the following grammar.
#+begin_example

e  := x        (where x is a variable)
   | e₁ e₂     (e₁ and e₂ are expressions and this the application rule)
               (Application)
   | λ x . e   ( where x is a variable and e is an expression)
               (Abstraction)


#+end_example

**** Application
The understanding is that the expression ~e₁ e₂~ should read as ~e₁~ applied on ~e₂~.
We will follow the conventions of Standard ML.

~e₁ e₂ e₃~ is the same as ~((e₁ e₂) e₃)~. That is "application"
associates towards the left

You can think of the "space" as the apply operator

x +ₗ y +ᵣ z     = ((x +ₗ y) +ᵣ z)  : Associates towards the left
x *ₗ y *ᵣ z     = x *ₗ (y *ᵣ z)    : Associates towards the right

**** Abstraction.

λ x . e should be read as that function that takes x to e (e will have x in it).

fn x => e

λ x . x
λ x . λ y . x  (The K function or constant function )

K takes an x and gives that constant function
that always gives out x.


Church gave λ-calculus as a way to study substitution in mathematics
formally. Calculus in this setting means a "set of rules"

λ x . λ y . e   will be written  λ x y . e



*** Semantics of the language.

**** Free and bound variables.

Idea is any variable that does not occur in the "shadow of a λ" is a
free variable.

λ x . x t

The variable x is /bound/ in the above expression where as the variable t is
free.

How to formally define what are the free variables of a λ-calculus expression.

FV : λ-calculus expressions -> Subset of Variables.

FV ( x     )   = { x }
FV (e₁ e₂  )   = FV(e₁) ∪ FV(e₂)
FV (λ x . e)   = FV(e) ∖ { x }

   FV (λ x . x t) = FV(x t) ∖ { x }
                  = (FV (x) ∪ FV (t)) ∖ { x }
                  = ({x} ∪ {t}) ∖ {x}
                  = {t}

Variables that occur in e but are not free are called bound.

(λ x₁ . x₁) x     x₁ is bound but x is free

Free/bound really depends on the position where the variable occurs.

**** Bound variables can be renamed but not free variables.

e = (λ x . x t) ≡ λ y . y t ≢ λ t . t t

e is that function which takes x to x t
e is that function which takes y to y t

- Change of Bound variables :: I am allowed to change the bound
  variables (with some care) without changing the meaning of the
  expression.

I should not change x to t because in the expression e the t was free,
where as if I change x to t then all the t becomes bound.

Variable capture.


(λ t . λ x . x t). I still cannot change x to t because the binding of t
changes.


- Change of Bound variables (safe version) :: I am allowed to change the
  bound variable to any fresh variable. A fresh variable is a variable that
  does not occur in the expression either as free or bound.

  The safe version of change of bound variables is called α-reduction
  or α-conversion.

₀∫∞  f(x, t) dx ≡  ₀∫∞  f(y, t) dy  ≢ ₀∫∞ f(t, t) dt


The main idea behind studying substitution carefully is to avoid
problems like the above.


**** Substitution.

e [ x := e']  :: Substitute e' for every /free/ occurance of x in e.

y [ x := e'] =  e'   if x = y
             =  y    otherwise


(e₁ e₂) [x := e']   = (e₁ [x := e']) (e₂ [x := e'])

(λ x . e) [x := e'] = (λ x . e)
(λ y . e) [x := e'] = λ y . e [x := e']  whenever y ≠ x







**** β-reduction (semantics)

(λ x . e) N ====> e [ x := N ]  (careful with variable capture)


M = λ x . e

M is that function that takes x to e

M N  = (λ x . e) N  (that function that takes x to e(x)) applied on N

e[x := N]

To avoid captures we might have to rename x in (λ x . e) to some fresh
variable before we do reduction.



(λ x . λ t . x t) t

M = λ x . (λ t . x t) ≡ λ x . (λ y . x y)
N = t

M N ===> λ t . N t  = λ t . t t

In M N the variable t is free whereas in the reduced form the variable is not free.


*** Church-Turing hypothesis is

λ calculus with β-reduction is as powerful as any other computation
model.


* Simply typed lambda calculus.

** Recap of untyped lambda calculus

#+begin_example

e = x        (x, y z etc are variables)
  | e₁ e₂
  | λ x . e

#+end_example

- e₁ e₂  ::  denotes the expression e₁ (which
  is some function) applied on the argument e₂.

- λ x . e :: denotes that function that takes
  the argument x and produces e.


Semantics is captured by what is known as β-reduction

(λ x . M) N --------->    M [ x := N ]

M [ x := N ] denotes the expression obtained by
replacing every "free" occurance of x in M with
the expression N

M = x

λ x . M = (λ x . x)


(λ x . (λ x . x) ) x

#+begin_src C

  char c = 'a';  /* global c */

  int main()
  {
     int c;         /* local c */

     .... c /* This is local c */

       ...;

     for(int c = 10 /* loop c */

           ; c < 100; ++c)
     {
         c /* this is loop c */
     }


  }

  void bar()
  {
    ....c ....  /* This is  global c */
  }

#+end_src


*** Untyped lambda calculus can do whatever Turing machines can do.

Church-Turing hypothesis.

*** The language is Untyped or Unityped.
** Simply Typed calculus.

Typed lambda calculus is the same lamda calculus which are
well types with respect to the typing rules.

*** Intuition
#+begin_example
e = x        (x, y z etc are variables)
  | e₁ e₂
  | λ x . e

#+end_example

In each of the above three cases, we have to distinguish between ill-typed and well-typed
expression.

The various typed λ-calculi they all are built on top of the untyped lambda calculus.
The simplest typed λ-calculi is the simply typed lambda calculus.


- For a variable x what is the type

- e₁ e₂ ::

  The type system should enforce that e₁ : function type whose domain is the same as the
  type of e₂.

- λ x . e :: Some how one needs to assign a functional type.

*** What are the types ?

- Base types :: A fixed set B = { nat , bool } of basic types.
- The types are given by the following inductive definition.
  (I will use the letter τ with subscripts to denote types)

  #+begin_example

  τ := b              b ∈ B  Any basic type b is a type
    | τ₁ -> τ₂               The type of functions from domain τ₁
                             to range τ₂

B = { bool , nat }

Examples of types over B.

bool
nat
bool -> nat
bool -> bool
(bool -> nat) -> nat

  #+end_example

*** Typing Rules.

Now we give a set of rules by which you can assign types to lambda
calculus terms (or you can decide that the lambda calculus term is ill
typed.

#+begin_example
e = n        for every n ∈ ℕ
  | true             (builtin constant true)
  | false            (builtin constant false)
  | and
  | not
  | plus
  --------------------------------------------------
  | x        (x, y z etc are variables)  (VAR)
  | e₁ e₂                                (APP)
  | λ x : τ . e  (Every abstraction should mention the type of the abstracted variable) (ABS)

#+end_example

Typing rules do not give you type inference. It merely says what is the type of a given
expression or decides that the expression is ill-typed.


There should be a typing rule for each construct of your language

**** How do I express typing rules ?

My logical judgements are of the form e : τ.  The expression e is of type τ.


1. I cannot say anything about the value of an expression e without knowing the
   value of the free variables in it.  x + 1,   fn x => x + 1

   - Types are in some sense an "approximation" of the values.
   - Types are specifications of values.

2. You cannot say anything about the type of e unless you specify the types
   of all free variables in e.

   We will have to make the judgement e : τ in the context where its free variables
   have already been assigned a type.

   We have Γ which is a typing environment. A typing environment assigns types
   to variables.

   Γ ⊢ e : τ  :: In the typing environment Γ, e has type τ.

   { x₁ : τ₁, x₂ : τ₂ .... , xₙ : τₙ } ⊢ e : τ



Γ = { x₁ : τ₁ , ...... , xₙ : τₙ } ; A set of type assumptions or you
can think of it as a partial function from the variable set to the set
of types.  To deduce the type xᵢ ∈ dom (Γ) just to mean { xᵢ : τᵢ } ⊂
of Γ

Γ ⊢ e : τ  is the judgement that in the typing context Γ, we can deduce e: τ.

- Important note :: Γ ⊢ e : τ can only be deduced if FreeVars(e) ⊆ dom
  Γ.  Even when FV(e) ⊆ Γ, it is possible that e is ill-typed and
  hence the judgement Γ ⊢ e : τ cannot be deduced.


- Two important questions ::

  + Type checking problem :: Given Γ, e and τ check whether Γ ⊢ e : τ

  + Type inference problem :: Given Γ, e find a τ (if possible) such
    that Γ ⊢ e : τ. The algorithm should as much as possible fail only
    if e is ill-typed.


#+begin_center

Pre-conditions
=============================
 Conclusion

#+end_center


#+begin_example

 n ∈ ℕ                   If I have the pre-condition that n is a natural number
====================
  n : nat                Then I can conclude n as a λ-calculus expression is of
                         type nat.

#+end_example



#+begin_example

                         No pre-condition
====================
  true : bool            I have the conclusion true : bool

#+end_example

Ex. Give the typing rules for false and plus

#+begin_center

==========================
plus : nat -> nat -> nat      (remember -> associates towards right)

#+end_center


*** VAR rule


is only possible in a typing context Γ where all free variables
have been assigned types.

#+begin_example

                             No pre-condition
=======================
 Γ ∪ { x : τ } ⊢ x : τ       In the environment where x has type τ, I can conclude x has type τ.

#+end_example

*** APP rule

What should this rule enforce ?

1. e₁ should be a function type of domain say α and range β; i.e   e₁ : α -> β


2. e₂'s type should be the domain type of e₁ ; e₂ : α



#+begin_center

Γ ⊢ e₁ : τ₁ -> τ
Γ ⊢ e₂ : τ₁
=====================
Γ ⊢ e₁ e₂ : τ


#+end_center

To apply the APP rule you need to already have concluded the typing judgements e₁ : τ₁ -> τ
and e₂ : τ₁. If your expression e is got by the APP rule, i.e. e = e₁ e₂ then the only way
e can be well typed is to make sure that e₁ : τ₁ -> τ and e₂ is of type τ₁ for some types
τ₁ and τ₂

*** ABS rule


#+begin_center

Γ ∪ { x : τ₁ } ⊢ e : τ₂
===================================
Γ ⊢ λ (x : τ₁) . e      : τ₁ -> τ₂


Suppose that in an environment where x : τ₁ I conclude that e : τ₂ then

=======================================================================

In the environment  (without the assumption on x) I can conclude (λ x : τ₁ . e ) : τ₁ -> τ₂.


#+end_center


*** Structural rules

The structural rules look silly but there is an interesting case of
type lambda calculus which does not have the structural rules (linear
lambda calculus)

**** Weakening.

Γ ∪ {x : τ} you should assume that Γ does not say anything about x.

#+begin_example

 Γ ⊢ e : τ
 =====================
 Γ ∪ {x : τ₁} ⊢ e : τ

#+end_example

**** Contraction

 Γ ∪ { x : τ₁ , x : τ₁ } ⊢ e : τ
 ===============================
 Γ ∪ {x : τ₁}            ⊢ e : τ




* Hindley Milner type system.


 Consider this type ~(α -> β) -> α list -> β list~. We have a polymorphic
 type here and the understanding is that the type variables ~α~ and ~β~ can be
 uniformly replaced by appropriate types.


 So a type like the one above in the context of Standard ML is
 actually more precisely described by explicit quantification, i.e. the
 above polymorphic type is expressed as

 ~∀ α ∀ β . (α -> β) -> α list -> β list~

 That is there is an implicit ∀-quantification over all type variables
 in the polymorphic type.


 The Hindley-Milner type system captures such polymorphic types. Note that
 quantification is /only allowed/ at the outer most layer. For example,

 ∀ α ∀ β . α -> β -> α is okey but not ∀ α . α -> (∀ β .  β -> α))

 What this means is that whenever you substitute type variables you
 need to substitutes all of them in one go.


 There is a reason for not allowing the second kind of types. Type
 checking as well as type inference with such general types (the
 corresponding lambda calculus is called System-F) is undecidable.

** Monotypes

I will use τ with possible subscripts to denote mono-types. This is
exactly like types in the simply typed lambda calculus but for type
variables.

#+begin_example

τ := b          b ∈ B the set of basic type
  |  α          α ∈ Type variable  (in addition to what you have for simply-typed calculus)
  |  τ₁ -> τ₂   where τ₁ and τ₂ are mono-types.

#+end_example

- Point of confusion :: In SML, when I write a type containing type
  variables you immediately think of the associated type scheme where
  all the type variables are quantified. In other words there are no
  mono-types with type free type variables in the SML top level.

  When talking about type inference algorithm and rules we need to be
  more precise on this. So only when type variables are quantified are
  the types considerd poly types (type schemes is a better word)
  otherwise they are treated as mono-types.

  #+begin_src sml

     fun foo x = let val y = (x,x)
                 in
                     (y,y)
                 end

    (*  Figuring out the type of foo.

        Start by an assumption x : nat
        See that in the rhs the type y : nat * nat
        and hence (y,y) is of type (nat * nat) * (nat * nat)

        and hence foo : nat -> (nat * nat) * (nat * nat)

        This is how one would have infered the type of foo in
        simply-typed lambda calculus

       If I had started with the assumption x : bool I would have ended
       with foo : bool -> (bool * bool) * (bool * bool)

       If I had started with the assumption x : α, whatever mono-type it
       is, I would have ended with foo : α -> (α * α) * (α * α)


       Let α be any type in the simply-typed lambda calculus.

        Starting by an assumption x : α

        See that in the rhs the type y : α * α
        and hence (y,y) is of type (α * α) * (α * α)

       (fixed but arbitrary)

     ,*)

  #+end_src

  #+RESULTS:
  : val foo = fn : 'a -> ('a * 'a) * ('a * 'a)

  In the language of Hindley-Milner type system I should really be saying

  ~foo : ∀ α . α -> (α * α) * (α * α)~ which is a type-scheme and not a mono-type.


** Type schemes.

Essentially type schemes are monotypes where some of the types are
∀-quantified in the outermost layer.

I will use σ with appropriate subscript to denote type schemes.

#+begin_example

σ = τ           for any monotype τ
  | ∀ α . σ'    for some type scheme σ'

#+end_example

i.e any type scheme looks like ~∀ α₁ ∀ α₂ .... ∀ αₙ . τ~ for some mono-type τ.


** Typing rules

Take all the typing rules for simply typed lambda calclus with all
types being mono-types. These rules continue to hold for
Hindley-Milner type system. There are two additional rules.


For example the VAR rule can be written as

#+begin_example

=================   for all mono-types τ.
{ x : τ } ⊢ x : τ

#+end_example


*** Specialisation

Γ ⊢ e : ∀ α . σ
===================
Γ ⊢ e : σ [ α := τ]  for some mono-type τ.


- Note :: Only monotypes are allowed to be substituted for type
  variables. Why ?

Suppose we have the judgement

1. e : ∀ α . α -> α

2. If Specialisation is allowed for all type schemes then consider
   replacing α by ∀ γ . γ -> γ. Then we will be able to get e : (∀γ
   . γ -> γ) -> (∀ β . β -> β) which is clearly not a Hindley Milner
   type.


Let us look at he more principled reason.


What is the understanding of the judgement  e : ∀ α . τ ?

No matter what monotype α is I can derive the type the type τ for e.
I am allowed to replace α consistently throughout τ with an arbitrary monotype
τ₁

If I apply specialisation repeatedly for the type scheme ∀ α₁ ... ∀ αₙ τ

Γ ⊢ e : ∀ α₁ ... ∀ αₙ . τ
=====================================
Γ ⊢ e :  τ [α₁ := τ₁] ... [αₙ := τₙ]  for monotypes τ₁,...,τₙ

*** Generalisation


Γ ⊢ e : σ
α is not free in Γ (side condition)
===================
Γ ⊢ e : ∀ α . σ

The side condition implies that there is no "fixing" of alpha done in the Γ

**** Explanation behind the side condition.
Imagine in the pre-condition Γ ⊢ e : σ  let us say that α is a type variable
that occurs in σ.

- α is Free in Γ :: i.e. there is an assumption { xᵢ : σᵢ } ⊆ Γ where α is free in σᵢ



   { x₁ : σ₁ .... , xᵢ : σᵢ(α) , ... } ⊢ e : σ  . In this case the α is fixed by the assumption

   xᵢ : σᵢ

   In this case generalisation is not allowed.

- α is /not/ Free in Γ :: i.e. there is an assumption { xᵢ : σᵢ } ⊆ Γ

  In this case you can generalise.


*** A simple type derivation.

#+begin_example
id = λ x . x
#+end_example

I want to give all possible types for id. (id : ∀ α . α -> α)


id : nat -> nat


1. x : bool ⊢ x : bool                    (By VAR rule)
2. ⊢ (λ (x : bool) . x) : bool -> bool    (By ABS rule)



Fix any α

1. x : α  ⊢ x : α               (BY Var rule)
2. ⊢ λ (x : α) . x : α -> α     (By ABS rule)
3. ⊢ λ x . x : ∀ α . α -> α     (BY GEN rule)



*** What are well typed program under type assumptions Γ?

A /proof/ of a typing judgement Γ ⊢ e : σ is a finite sequence of
Judgements J₁, ... , Jₙ where Jₙ is the judgement Γ ⊢ e : σ and for
every judgement Jᵢ there is a typing rule R such that The
pre-conditions of R is contained in the set {J₁,...,Jᵢ₋₁ } and Jᵢ is
the consequence of R.


An expression e is well typed under the context Γ if there exists a
type scheme σ such that Γ ⊢ e : σ has a proof using the typing rules.


*** Type inference algorithm in this context.

- Input :: Γ and e
- Output ::  σ such that Γ ⊢ e : σ is derivable using the typing rules.
  or error if e is ill-typed in Γ.
- Additional requirement/property of the algorithm is that it computes
  the /most general type/ that is possible to be assigned to ~e~


What is the type of ~λ x . x~ ? Its type is (nat -> nat), Its type is
(nat -> bool) -> nat -> bool or in other words you can say that its
type is τ -> τ for all monotypes. Or in other words you can say that
it has type ~∀ α . α -> α~

Clearly (nat -> nat) is a specialisation of ∀ α . α -> α. SO the
latter is more general than the former. The algorithm we design should
compute the most general type possible.


We define an pre-ordering ≤ on type schemes. A pre-order is a
reflexive, transitive relation.

pre-order + anti-symmetry = partial order.


i.e a ≤ a (reflexive) , a ≤ b and b ≤ c implies a ≤ c (transitive)


We say that σ₁ ≤ σ₂ if σ₁ is a special case of σ₂.

You should think of the relation σ₁ ≤ σ₂ as saying σ₁ is less general
type than σ₂.


Suppose we have  type scheme σ = ∀ α₁ .... αₙ . τ. Let us look at monotypes alone.

1. Clearly τ' := τ [ α₁ := τ₁] .... [αₙ = τₙ]  is a special case of σ or τ' ≤ σ.

2. Let β₁,....,βₘ be any set of type variables such that βᵢ is /not free/  in σ.

   Then any type scheme σ' = ∀ β₁ .... ∀ βₙ . τ' where τ' = τ' := τ [
   α₁ := τ₁] .... [αₙ = τₙ] is less general than σ.


In other words what ever type scheme you get form sigma by the following two steps

- monomorphise ::  Compute the mono-type τ' = τ' := τ [ α₁ := τ₁] .... [αₙ = τₙ]

- Quantify :: Compute the σ' =  σ' = ∀ β₁ .... ∀ βₙ . τ' for βᵢ not free in σ



Let us look at an example


- σ₁ = ∀ α . α -> α  = ∀ α . τ₁



- σ₂ = ∀ β . (β -> β) -> (β -> β)

  σ₂ can be obtained by the following steps.

 1. τ' = replace α with the monotype β-> β in τ₁ to get (β -> β) -> (β -> β)
 2. Then quantify over β because in τ' that is not free in the original type scheme σ₁


- σ₃ = ∀ β ∀ γ . (β -> γ) -> (β -> γ)


  1. Replace α with (β -> γ) in τ₁
  2. Quantify over β and γ.



What mono-types are specialisation of σ₃

Any monotype  (τ₁ -> τ₂) -> (τ₁ -> τ₂). Or
any monotype that looks like
(something -> someting else) -> (the same something -> the same something else)

( something -> something

- σ  = ∀ α₁ ∀ α₂ ... ∀ αₙ .

          (α₁ -> α₂ -> .... -> αₙ)
       -> (α₁ -> α₂ -> ...  -> αₙ)

- σ₄ = ∀ β . (β -> β) -> β

  Answer the following :
- σ₂ ≤ σ₁ ? :: Yes
- σ₃ ≤ σ₁ ? :: Yes
- σ₄ ≤ σ₁ ? :: No
- σ ≤ σ₁    :: Yes


In other words what we are really interested is the "shape" of the type scheme.


Closure(σ) = the type scheme obtained by quantifying over all the type variables
of σ.

σ and look at all free variables let us say they are α₁ .... αₙ  Then

Closure(σ) = ∀ α₁ ... ∀ αₙ σ


If the starting type scheme σ is closed, then the specialisations of σ are



Exercise that you can try to prove.

Form the rules of specialisation and generalisation
prove the following.

Suppose that Γ ⊢ e : σ is provable and σ' ≤ σ then Γ ⊢ e : σ' is also
provable.



#+begin_example

 Γ ⊢ e : σ

 σ' ≤ σ
===================
 Γ ⊢ e : σ'


 The generalised specialisation rule. If I can deduce e : σ then I can deduce e : σ' for all
 specialisation σ' of σ.


#+end_example

Remember that σ' ≤ σ if there is a τ' such that

(1) τ' = τ [ α₁ = τ₁ ] .... [αₙ = τₙ ]  where σ = ∀ α₁ ... ∀ αₙ τ.

Repeated use of the specialisation rule should give you Γ ⊢ e : τ'
To get to Γ ⊢ e : σ' I almost need to use the generalisation rule multiple times.


* Type inference algorithm

** Informal description.

#+begin_src  sml


  val foo = fn x => fn y => x

  (*

   The algorithm should figure out that foo : σ where the judgement ⊢ foo : σ should be
   derivable from the rules of typing.

  1. We first figure out a τ such that foo : τ and Closure(τ) is the most general type
     that we can assign to foo.

     Closure(τ) = ∀ α₁ ... ∀ αₙ . τ  where FV(τ) = { α₁ ... αₙ }



   Let us pick a fresh variable α



   1. ...
   2. ...
   3. .....
      { x : α₁ } ⊢ e : β
      =====================
      foo = λ x . e : α₁ -> β


  ,*)


#+end_src

#+RESULTS:
: val foo = fn : 'a -> 'b -> 'a

~foo : ∀ α ∀ β . α -> β -> α~


** Inference algorithm

Closure(Γ, τ) = ∀ α₁, .... , ∀ αₙ τ where FV(τ) ∖ FV(Γ) = {α₁,....,αₙ}

- Input :: Type assumption Γ, and an expression e

- Output :: If possible a monotype τ such that Γ ⊢ e : Closure(Γ, τ)
  is derivable by the type rules and if Γ ⊢ e : σ is some other type
  scheme which is provable then σ ≤ Closure(Γ, τ)

  We want to compute the monotype τ such that Closure(Γ, τ) is the
  most general type that we can assign to e under the typing
  assumptions Γ.


The generalisation rule can be extended as follows


Γ ⊢ e : τ
====================  (Extended generalisation rule)
Γ ⊢ e : Closure(Γ, τ)

Suppose under Γ the most general type scheme that you can deduce for e
is σ = ∀ α₁ ... ∀ αₙ . τ

...
...
...

-> (m)    Γ ⊢ e : τ
(m+1)  Γ ⊢ e : ∀ αₙ . τ  (Gen m)
(m+2)  ....
(m+n)  Γ ⊢ e : ∀ α₁ .... ∀ αₙ . τ (Gen m + n - 1)



Take all the variables that are free in τ but not in Γ and do
generalisation some many times.  That will give you the extended
generalisation rule.


Γ = { x₁ : σ₁ ; .... , xₙ : σₙ }  FV(Γ) = FV(σ₁) ∪ FV(σ₂) .... ∪ FV(σₙ)


| Assumptions        | Infer               | Constraint     | Rule used |
|--------------------+---------------------+----------------+-----------|
|                    |                     |                |           |
| { }                | (λ x . λ y . x) : α | {}             | ABS       |
|--------------------+---------------------+----------------+-----------|
| { x : α₁ }         | (λ y . x) : β₁      | α ≡ α₁ -> β₁   | ABS       |
|--------------------+---------------------+----------------+-----------|
| { x : α₁ ; y : α₂} | x : β₂              | α ≡ α₁ -> β₁   | VAR       |
|                    |                     | β₁≡ α₂ -> β₂   |           |
|--------------------+---------------------+----------------+-----------|
| { x : α₁ ; y : α₂} | {}                  | α ≡ α₁ -> β₁ ; |           |
|                    |                     | β₁≡ α₂ -> β₂ ; |           |
|                    |                     | β₂≡ α₁         |           |
|                    |                     |                |           |



If I solve for this constraints I get the equations.

α  ≡ α₁ -> (α₂ -> α₁)
β₁ ≡ α₂ -> α₁
β₂ ≡ α₁


1. {x : α₁ ; y : α₂ } ⊢ x : α₁          (VAR Rule + Weakening)
2. {x : α₁ } ⊢ λ y . x : α₂ -> α₁       (From 1 with ABS)
3.  ⊢ λ x . λ y . x : α₁ -> (α₂ -> α₁)            (From 2 with ABS)
4. ⊢ λ x . λ y . x : ∀ α₁ ∀ α₂ . α₁ -> (α₂ -> α₁) ( Generalisation 2 times )


What we need is an algorithm to solve set of typing equations which looks like


τ₁₁ ≡ τ₂₁ ; τ₂₁ ≡ τ₂₂ ....  τₘ₁ ≡ τₘ₂

#+begin_example sml

val foo = map (fn x => plus x 1)

(*

   foo : α list -> β list
   foo : int list -> int list
   where fn x => plus x 1 should have type α -> β
   {x : α } ⊢  plus x 1 : β

   β ≡ int
   α ≡ int

*)
#+end_example



The free variables here are map and plus.

I can infer the type of foo only in the context where map and plus has been assigned some type.

Γ = { map : ∀ α ∀ β . (α -> β) -> α list -> β list ; plus : int -> int -> int }






** Inference algorithm reduces to Unification.

- Input :: Γ and e

- Output :: If possible compute the monotype τ such that Γ ⊢ e : τ and
  the Closure(Γ, τ) is the most general type that can be assigned to
  e.

The algorithm is a recursive algorithm that recurses on the structure
of e.

#+begin_example ascii

e :  x         a variabe
  | e₁ e₂      application
  | λ x . e    abstraction

#+end_example

It assigns types to all variables that are occuring in e. We can
assign types to all subexressions in the expression e.

1. For each subexpression it assigns a monotype τ

2. It computes a set S of /type unification/ constraints S = { τ₁ ≡
   τ₁' , ...., τₙ ≡ τₙ' }.

   A set W of /type variable assignment/ { α₁ := κ₁ , ...., αₘ := κₘ }
   where αᵢ's are type variables and the κᵢ are monotypes that do not
   contain {α₁,...., αₘ }, is a solution to the type unification constraints S if.
   For every constraint τ ≡ τ' ∈ S we have τ[W] = τ'[W]

   τ[W] is the monotype τ[α₁ := κ₁]....[αₙ := κₙ] got by
   simultaneously substituting αᵢ with κᵢ for all assignments αᵢ := κᵢ
   ∈ W.

3. The expression e is well typed under Γ if and only if  S can be /solved/.

   i.e. If the computed constraints can be solved then e is well typed
   and If it cannot be solved then e is ill-typed.

4.  If W is a solution to the constraint that has been computed and let τ be the
   monotype that has been assigned to e then the desired type for e is τ[W].

5. As a result, all we need as a subroutine is a way to solve the type
   unification problem.


Since an expression is of one of the below three forms we have three cases in the
type inference algorithm
#+begin_example ascii

e :  x         a variabe
  | e₁ e₂      application
  | λ x . e    abstraction

#+end_example

- VAR Case :: We already have a typing assumption Γ with us.
  Suppose the input expression e is actually x for some variable x.

  1. There is a typing assumption (x : σ) ∈ Γ . I have to compute a
     monotype for τ for e (which happens to be the variable x).

     Suppose σ = ∀ α₁...∀ αₙ . τ . Then compute /fresh variables/
     β₁,...., βₙ and associated the subexpression x with the type
     τ' = τ[α₁:= β₁] ....[αₙ := βₙ]

  - Question :: We have assigned the monotype τ' for x , Is it the case
      that  Closure(Γ, τ') is the most general type that we can associated for
       the expression x under Γ ?

       - Answer is that yes it is be most general type
         we can associated for x because.

         Closure(Γ,τ') =

         σ' ∀ β₁,....∀ βₙ τ [α₁ := β₁] .....[αₙ := βₙ]

         x : σ ∈ Γ

         Note that σ' is essentially σ (because only a bound variable
         rename has been done).

         The output is (∅  , e : τ') , i.e. empty set of constraints and
         for e the type is τ'

  2. There is no typing assumption for the variable x in Γ, x ∉ domain(Γ)
     The algorithm fails with an error message "undefined variable x"


- APP case ::

  The input expression e := e₁ e₂

  We pick fresh type variables α and β. Recursively run the inference algorithm on
  Γ, e₁ and Γ, e₂

  1. (S₁, e₁ : τ₁) := Infer(Γ, e₁)

  2. (S₂, e₂ : τ₂) := Infer(Γ, e₂)

     The output is (S₁ ∪ S₂ ∪ { α -> β ≡ τ₁ , α ≡ τ₂ }, e₁ e₂ : β )
     For some fresh variables α and β. The point of this step is to
     enforce that τ₁ looks like something -> something else where τ₂
     is also something.

- ABS case ::

  The expression e = λ x . M.

  Recursively infer (S, M : τ) := Infer(Γ ∪ (x : α), M)

  Then Output is (S , λ x . M : α -> τ) For some fresh variable α.


#+begin_example sml

val foo := map incr

Γ := { map : ∀ α ∀ β, (α -> β) -> α list -> β list ; incr : int -> int }, foo = (map incr) }

1. We are in the VAR case where e = map incr.

   (S₁, map : τ)    := Infer (Γ, map)
   (S₂, incr : τ)   := Infer (Γ, incr







#+end_example


1. Start with Infer (Γ, map incr)

   Use the Apply case and recursively compute the type of each sub expression map and incr

   Result will be {  (α₃ -> α₄) ≡ (α₁ -> α₂) -> (α₁ list -> α₂ list))
                     α₃         ≡ (int -> int)
                   },


                   map incr : α₄

                   α₃ ≡ (α₁ -> α₂)
                   α₄ ≡ α₁ list -> α₂ list
                   α₃ ≡ int -> int

                   α₁ ≡ int
                   α₂ ≡ int
                   α₃ ≡ int -> int
                   α₄ ≡ int list -> int list

   - Infer(Γ, map)

     Use the Var case. map : ∀ α ∀ β (α -> β) -> (α list -> β list)

     Result ( ∅ , map : (α₁ -> α₂) -> α₁ list -> α₂ list)

   - Infer(Γ, incr)

     Result ( ∅ , incr : int -> int)



* Type unification algorithm.

** Recap
Hindle-Milner type schemes.

σ = ∀ α₁ ... ∀ αₙ τ   where τ is a mono-type.

#+begin_example

τ = basic types
  | type variable
  | τ₁ -> τ₂     where τ₁ and τ₂ are themselves
                 monotypes.

#+end_example


Typing environment/ typing assumption Γ = { x₁ : σ₁, ...., xₘ : σₘ }
where xᵢ 's are variables (not type variables but variables of the
language } and σᵢ's are type schemes.


Given a typing assumption and an expression e the algorithm should

1. Find the most general type scheme σ such that Γ ⊢ e : σ if e is well-typed or

2. Should say that e is ill-typed under the assumptions Γ.


A general type inference algorithm reduces to the type unification
problem.



1. A special case of type inference (mono-type type inference problem)
   which is defined as follows.

   Given Γ and e compute the monotype (if possible) τ such that Γ ⊢
   e : τ and σ = Closure(Γ, τ) is the most general type scheme such
   that Γ ⊢ e : σ.

   Firstly if Γ ⊢ e : σ' then Γ ⊢ e : Closure(Γ, σ'), clearly it is
   sufficient to solve the mono-type inference for e (Easy, use the
   generalisation rule multiple times).

   If α is free in σ' and not free in Γ, i.e. α ∈ FreetypeV(σ') ∖ FreetypeV(Γ) then

   #+begin_example
   Γ ⊢ e : σ'
   ================  (one step of GEN)
   Γ ⊢ e : ∀ α . σ'


#+end_example


The monotype inference algorithm has three cases (based on the structure of the
expression)


1. e is a variable x :: Suppose x : σ ∈ Γ then the type inferred for e is τ where
   τ is obtained by specialising σ by fresh type variables.

   x : ∀ α ∀ β ∀ γ . τ


   e : τ [α := α'] [β := β'] [γ = γ'] where α', β' , γ' are all fresh variables.

   The set of constraints on generated is ∅


2. e = e₁ e₂

   Generate fresh variables α, β such that ?

   let (S₁, τ₁) = Infer(Γ, e₁)

       (S₂, τ₂) = Infer(Γ, e₂)


   We want to ensure that τ₁ is τ₁' -> τ₂'  and τ₂ should be τ₁'

   Result := (S, τ) where
   S = S₁ ∪ S₂ ∪ { τ₁ ≡ α -> β ; τ₂ ≡ β }
   τ = β


3. e = λ x . e' then

   (S, τ) = Infer(Γ, { x : α } , e')

   Result = (S, α -> τ)




The final result of the algorithm :

- We have got a set of constraints S and a type τ such that

- The final monotype that we want to assign e is just τ[S]


In general the constraints S = { τ₁ ≡ τ₁' , τ₂ ≡ τ₂' ..... τₘ ≡ τₘ'
}. We want to solve the constraints above.


- Substitution :: A set of type variable replacement { α₁ := κ₁ ;
  ...... αₘ := κₘ } where αᵢ's are type variables and κᵢ's are
  monotypes with the property that FV(kᵢ) ∩ { α₁ , .... ,αₘ } = ∅


- Solution for an equation τ ≡ τ' :: A substitution T such that if
  τ[T] = τ'[T]

  e.g. For the equation α -> int ≡ char -> β { α := char, β := int }
  is a solution.

- Solution for a set S of equations :: A solution T that is simultaneously
  the solution for every equation τ ≡ τ' ∈ S

- Among the solutions we can find the most general solution.

  α -> γ ≡ (α₁ -> β₁) -> α

  α := α₁ -> β₁  γ := α₁ -> β₁    (most general solution)

  α := int -> β₁  γ := int -> β₁  (some solution)



*










** Type unification problem.



Sometimes solving a "harder problem" is "easier". Solving a more
general problem is often easier than solving a specific problem.


1. Given as input two monotypes τ₁ and τ₂ find the most general unifier (i.e the substitution T
   that makes both τ₁ and τ₂ the same.) if possible.

   τ₁ = α -> β    τ₂ = (α -> γ) -> β

   α ≡ α -> γ    α := α -> γ  (not a solution)


2. Given a finite set of equations S = { τ₁ ≡ τ₁' , .... , τₙ ≡ τₙ' } find the most general
   unifier for S. The first problem is a special case of the second S = { τ₁ ≡ τ₂ }


3. Given an equation τ₁ ≡ τ₂ and a substitution T = { α₁ := κ₁, ... , αₘ := κₘ }, Find
   the most general unifier for the equation τ₁ [T] ≡ τ₂[T]


4. The variant of 3 where we deal with multiple equations. Given S = { equations } and
   a substitution T, find the solution for S = { τ₁[T] ≡ τ₂ [T} | τ₁ ≡ τ₂ ∈ S }

Why I need (2) instead (1).


The solution for (τ₁ -> τ₂) ≡ τ₁' -> τ₂' is the same as the solution for the set

S = { τ₁ ≡ τ₁' , τ₂ ≡ τ₂' }



Why I need 4.

S = { τ₁ ≡ τ₁' } ∪ S'

Let T be a solution of τ₁ ≡ τ₁'. Then a solution for S is just a
solution for S' under the substitution T


*** Type Unification problem.

- Input :: A set S of equations and a substitution T
- Output :: The most general substitution R such that
  τ[T] [R] = τ'[T] [R] for all equations τ ≡ τ' ∈ S.



Type unification to type inference.

Recall that the algorithm computes a set S of constraints (equations) and a type τ
for e. The final result of the type inference algorithm is τ[T] where T is the solution
for the type unfication problem for S.

*  Old notes starts here


* DONE Types and Functions                                        :CLASSROOM:
SCHEDULED: <2020-08-17 Mon>

** Standard ML has types

It is strongly enforced.


|                       | Weak types | strong types |
|-----------------------+------------+--------------|
| Static type checking  | C, C++     | SML, Java    |
| Dynamic type checking | JavaScript | Ruby, scheme |


*** Static type checking

- Advantages ::

1. Errors are caught before deployment.
2. More efficient code is expected out of static type checking

- Disadvantages ::

1. Very verbose type declarations.
2. Not easy to prototype

These complaints are because of languages like Java.

SML has this nice property that it can infer the types.

#+BEGIN_SRC sml

fun add x y = x + y

#+END_SRC

#+RESULTS:
: val add = fn : int -> int -> int

** Strong types are good

Write a function to compute the average of two real numbers.

#+BEGIN_SRC C
# include <stdio.h>
double av( double x , double y)
{
   return 1/2 * (x + y);
}

int main ()
{
   printf("the result is %g\n", av(2,3));
}


#+END_SRC

#+RESULTS:
: the result is 0

The bug is due to the automatic conversion from
integers to double.

#+BEGIN_SRC sml

fun av x y = 1.0/2.0 * (x + y)


#+END_SRC

#+RESULTS:
: val av = fn : real -> real -> real



** What are types

- Basic types :: ~int~, ~bool~, ~char~, ~string~

#+BEGIN_SRC sml
val anInt = 10
val aBool = true
val anotherBool = false
val aChar = #"c"
val aString = "hello\n"

#+END_SRC

- Compound types :: product types, lists etc

#+BEGIN_SRC sml
val y = (1, "hello")
val y1 = (1,true, "hello")
val x  = #1 y
val h =  #2 y
val z = [1,2,3]
val z1 = [ "hello", "world"]
val u = 2 :: z
val u1 = []
val u2 = "foo" :: z1
fun bar x (y : real) = x + y
fun bar1 x y = (x+1,y)

(*
 A * B  is the type of all tuples (a,b) where a : A and b : B.

'a , 'b , 'c  --> type variables

A -> B denotes the type of functions from A to B

Functions whose range is B and domain is A

*)
#+END_SRC

#+RESULTS:
#+begin_example
val y = (1,"hello") : int * string
val y1 = (1,true,"hello") : int * bool * string
val x = 1 : int
val h = "hello" : string
val z = [1,2,3] : int list
val z1 = ["hello","world"] : string list
val u = [2,1,2,3] : int list
val u1 = [] : 'a list
val u2 = ["foo","hello","world"] : string list
val bar = fn : real -> real -> real
val bar1 = fn : int -> 'a -> int * 'a
#+end_example



- Polymorphism :: SML figures out the most general possible type.
This kind of polymorphism is called parametric polymorphism


** Function evaluation

*** Variable bindings
*** Reductions/simplifications

#+BEGIN_SRC sml

fun increment x = x + 1
(* the variable increment is bound to that function which on input x gives x + 1 *)
val x = 10
val z = increment (2 + x)

fun foo (x,y) = x + y

fun bar () () = ()

val u = bar (print "hello\n")


(*
    f            e
    increment (2 + x) ---> increment (2 + x)
                     ---> increment (2 + 10)
                     ---> increment 12 ----> bind x to 12 and evalute (x + 1 )
                     ---> 12 + 1
                     ---> 13
                       match it with the lhs (identity x) which binds x to 12
    (1) reduce f
    (2) reduce e
    (3)

Eger evaluation: Arugments are reduced before functions are applied.
lazy evaluation: Arugments are evaluated only when needed.

*)

#+END_SRC

#+RESULTS:
: hello
: val increment = fn : int -> int
: val x = 10 : int
: val z = 13 : int
: val foo = fn : int * int -> int
: val bar = fn : unit -> unit -> unit
: val u = fn : unit -> unit

1. First the RHS is reduced to a value and then
   bound to x

2. During evaluation if a variable is found then
   its corresponding bound value is substituted

* DONE Algebraic data types and Pattern Matching                  :CLASSROOM:
SCHEDULED: <2020-08-24 Mon>

** More types

1. Types i.e basic types like ~int~, ~real~, ~string~
2. Cartesian produce ~int * real~
3. Function types ~int -> string~
4. Polymorphism ~'a -> 'b -> 'a~
5. Type aliasing

#+BEGIN_SRC sml

type complex = real * real
val x : complex = (2.0, 1.0)

fun realpart (a:real, b:real) = a
val z = realpart x
(*
 complex is just a new name for real * real.
As types they are the same.

*)
#+END_SRC

#+RESULTS:
: type complex = real * real
: val x = (2.0,1.0) : complex
: val realpart = fn : real * real -> real
: val z = 2.0 : real

** Algebraic type.

#+BEGIN_SRC sml

datatype Day = Sun
             | Mon
             | Tue
             | Wed
             | Thu
             | Fri
             | Sat

(* If SML did not have booleans *)
datatype Bool = True
              | False

(* how to define a value of type Day *)
val x = Sun

(* how to write functions *)

(*

Write a function isHoliday : Day -> bool

1. d : Day
2. ... : bool

*)
fun isHoliday Sun = true
  | isHoliday Sat = true
  | isHoliday _   = false


(*


isHoliday Mon  ---->
  Try in this order

  1. match isHoliday Sun with isHoliday Mon ---> true
  2. match isHoliday Sat with isHoliday Mon ---> true
  3. match isHolida  _   with isHoliday Mon ---> false


*)
(*

This is not like enum of C or C++ because
there is not automatic conversion from enum
to int and vice-versa

*)

#+END_SRC

#+RESULTS:
: stdIn:107.5-107.25 Warning: match nonexhaustive
:           Sun => ...
:
: datatype Day = Fri | Mon | Sat | Sun | Thu | Tue | Wed
: datatype Bool = False | True
: val x = Sun : Day
: val isHoliday = fn : Day -> bool

#+BEGIN_SRC sml

val x = SOME 10
val y = NONE

(*

head is a function that takes a list and
produces the first element of the list.

head : 'a list -> 'a

head is not defined on empty list
THis will be a runtime bug because

head e

headSafe : 'a list -> 'a option


datatype 'a option = SOME of 'a
                   | NONE


print : string -> ()

*)

datatype 'a Option = Some of 'a
                   | None

fun head (x :: _) = x

fun headSafe (x :: _) = SOME x
  | headSafe _        = NONE


val foo = head [1,2,3]
val bar = headSafe []
val _   = print (head ["foo" , "bar" ])
val _   = print (head [])

val _   = print (headSafe ["foo", "bar"])
#+END_SRC

#+RESULTS:
: stdIn:233.5-233.22 Warning: type vars not generalized because of
:    value restriction are instantiated to dummy types (X1,X2,...)
: stdIn:237.5-237.42 Error: operator and operand don't agree [tycon mismatch]
:   operator domain: string
:   operand:         string option
:   in expression:

#+BEGIN_SRC C

int main ()
{
  FILE *fp;
  if ( fp = fopen("hello"))  == NULL)
  {
     /*
      handle the fact that there is no hello file */
  }
  ... stuff with fp

}

#+END_SRC
* DONE Abstract syntax and datatypes                              :CLASSROOM:
SCHEDULED: <2020-09-05 Sat>

** While writing compilers

1. Captures the constructs of the language

2. Depending on the construct translate the code to target code

3. Use the constructs of the language to (in an editor) highlight parts
   differently

4. Source code processing like indenting, linting,


Keeping the programs text is not very convenient.


1. Some syntax is just sugar
   e.g [1,2,3] is a sugar for 1 :: 2 :: 3 :: []

2. Program text needs to worry about things like precedence whereas
   the representation that we see need not worry.

** Abstract syntax of the language

This is a term used mainly in programming language theory.

- Parse tree  - compiler literature
- Abstract syntax tree

Consider the language of expressions with + and * and constants (integers)

#+BEGIN_EXAMPLE
1 + 2 * 3

(1 * 2) + 3

(1 + 2) * 3

1 +   -- not an expression
1 3   -- not an expression

(1 + 2) 3 -- not an expression

#+END_EXAMPLE

- Concrete syntax ::

1. Governs what string/text is valid program
2. We want to know "how" the string is a member of the language. x ∈ L
3. We need ways to ensure that the grammar is unambiguous
4. Used to convert from "textual" representation of
   the program to .... abstract syntax or parse trees


#+BEGIN_EXAMPLE

E -> nat    -- nat rule
  |  E + E  -- plus rule
  |  E * E  -- mul rule   has more precedence
  | ( E )   -- paren rule


Proof that 1 + 2 * 3 is an E  --- 1 + {2 * 3}

(1) 1 is an E  (nat rule)
(2) 2 is an E  (nat rule)
(3) 3 is an E  (nat rule)
(4) 2 * 3 is an E (mul using 2,3)
(5) 1 + 2 * 3 is an E (plus using 1,4)


Proof that 1 + 2 * 3 is an E   {1 + 2} * 3

(1) 1 is an E  (nat rule)
(2) 2 is an E  (nat rule)
(3) 3 is an E  (nat rule)
(4) 1 + 2 is an E (plus using 1,2)
(5) 1 + 2 * 3 is an E (mul using 3,4)

#+END_EXAMPLE


Expressing expressions as trees will not have a problem of ambiguity.

#+BEGIN_EXAMPLE

         1 + {2 * 3}          {1 + 2} * 3

             +		           *
            / \		          / \
           1   * 		       	 +   3
              / \		      	/ \
             2   3	               1   2


#+END_EXAMPLE


- Abstract syntax ::

#+BEGIN_EXAMPLE

E -> nat
   | E + E
   | E * E

#+END_EXAMPLE

1. A nat is an expression

2. If E1 and E2 are expressions then E1 + E2 is an expression

3. If E1 and E2 are expressions then E1 * E2 is expression

#+BEGIN_SRC sml
datatype expr = Const of int
              | Plus  of expr * expr
              | Mul   of expr * expr


fun exprDenote (Const x)      = x
  | exprDenote (Plus (e1,e2)) = exprDenote e1 + exprDenote e2
  | exprDenote (Mul (e1,e2))  = exprDenote e1 * exprDenote e2

(* instruction of a stack machine *)
datatype inst = push of int
              | plus
              | mul


(*

push x :  pushes x on top of the stack

plus : x1 = pop x2 = pop ; push (x1 + x2)

mul  : x1 = pop x2 = pop ; push (x1 * x2)


*)
type executable = inst list

type stack = int list

(* instDenote : inst -> stack -> stack *)
fun instDenote (push x) stk                 = x :: stk
  | instDenote plus     (x1 :: x2 :: stk)   = x1 + x2 :: stk
  | instDenote mul      (x1 :: x2 :: stk)   = x1 * x2 :: stk

fun interp prog stk = fold....

(* compile : expr -> executable *)
fun compile (Const x)       = [push x]
  | compile (Plus (e1,e2))  = let val prog1 = compile e1
                                  val prog2 = compile e2
                              in
                                 prog2 @ prog1 @ [plus]
                              end

  | compile (Mul  (e1,e2))  = let val prog1 = compile e1
                                  val prog2 = compile e2
                              in
                                 prog2 @ prog1 @ [mul]
                              end

val one = Const 1
val two = Const 2
val three = Const 3
val x = Plus (Plus (one, two), three)
val p1 = compile x
#+END_SRC

#+RESULTS:
: datatype expr = Const of int | Mul of expr * expr | Plus of expr * expr
: datatype inst = mul | plus | push of int
: type executable = inst list
: val compile = fn : expr -> inst list
: val one = Const 1 : expr
: val two = Const 2 : expr
: val three = Const 3 : expr
: val x = Plus (Plus (Const #,Const #),Const 3) : expr
: val p1 = [push 3,push 2,push 1,plus,plus] : inst list

Abstract syntax

1. Only captures the essence of the syntax
2. Brackets and other disambiguation things can be removed.


#+BEGIN_EXAMPLE

S -> nat
  |  nat , S

#+END_EXAMPLE

#+BEGIN_SRC sml

type commaSepNat = int list

#+END_SRC
* DONE Structures and functors                                    :CLASSROOM:

** Main Idea

Main idea :: Control the name-space of definitions.

1. Possibly multiple instances of name vs value binding

2. Together with functor it gives powerful way of manipulating name space.
#+BEGIN_SRC sml

val x = 10
val x = "hello"

structure A = struct
   val  x   = 10
   type foo = int
end
(*

Modules in ocaml.

*)

val y = A.x
fun myfun (x : A.foo) = x + 1

open A (* generaly discouraged *)
fun myanotherfun (x : foo) = x + 1

val u = List.map myfun [1,2,3]

#+END_SRC

#+RESULTS:
#+begin_example
val x = <hidden-value> : int
val x = <hidden-value> : string
structure A :
  sig
    val x : int
    type foo = int
  end
val y = 10 : int
val myfun = fn : foo -> int
opening A
  val x : int
  type foo = int
val myanotherfun = fn : foo -> int
val u = [2,3,4] : int list
#+end_example

Caution

1. Do not confuse this with structures in C. In C structs are just
   product types which in SML is called records.

2. Similar to namespace in C++

** Signatures


1. Signatures can used to control what is exposed from a structure.

2. Signatures itself can be defined and used

3. Signature is used in functors to control what structures are expected to have.


#+BEGIN_SRC sml

signature MYSIG = sig
   type foo
   val  x : foo
 end

structure A : MYSIG

(* sig
   val x : int
   type foo
  end *) =  struct

val x = 10
val y = 100
type foo = int
type bar = string
end

val y = A.x  (* this is fine *)
(* val z = A.y  (* not fine as y is not exposed *) *)


structure B : MYSIG = struct

  type foo = string
  val x    = "hello"

end

#+END_SRC

#+RESULTS:
: signature MYSIG =
:   sig
:     type foo
:     val x : foo
:   end
: structure A : MYSIG
: val y = 10 : foo
: structure B : MYSIG


** An analogy with values and types


| Value world            | Structure world                  | Ocaml        |
|------------------------+----------------------------------+--------------|
| values (val)           | structures (structure .. struct) | module       |
| types  (type/datatype) | signatures (signature .. sig)    | module types |
| functions (fun)        | functor                          | functor      |


** Functors take structures and produce other structure.

#+BEGIN_SRC sml

datatype CMP = LT | GT | EQ

signature ORD = sig
  type t
  val compare : t -> t -> CMP
end


structure IntOrd : ORD = struct
  type t = int
  fun compare x y = ...
end


functor Invert ( O : ORD ) = struct
   type t = O.t
   fun compare x y = O.compare y x
end

struct InvIntOrd = Invert (IntOrd)

functor Sort (O : ORD) = struct
   fun sort (xs : O.t list) = ....

end

#+END_SRC

#+RESULTS:
#+begin_example
datatype CMP = EQ | GT | LT
signature ORD =
  sig
    type t
    val compare : t -> t -> CMP
  end
functor Invert(O: sig
                    type t
                    val compare : t -> t -> CMP
                  end) :
              sig
                type t
                val compare : O.t -> O.t -> CMP
              end
#+end_example


** General facts

1. Structures/functors are unique to ML and its dialect. But they can
   be retrofitted to any language.

2. One can defunctorise the code, i.e. take a program with structurs
   and functors and rewrite it to get code that is without them.  In
   fact the first phase of MLton compiler is precisely this.

3. structure/functors are what are know as zero-cost
   abstraction. There is no cost at runtime (space or time) for using
   the feature structure/functors. Maybe compile time will increase.


* DONE Structures and funtors continued                           :CLASSROOM:


** Some interesting libraries

1. Look for utility functions in the Standard Basis library

#+BEGIN_SRC sml

open Array

val myintarray = array (10, 42)
val mystrarray = array (10, "The answer is")
val _ = update (myintarray, 0, 100)

#+END_SRC

#+RESULTS:
#+begin_example
opening Array
  type 'a array = 'a ?.array
  type 'a vector = 'a ?.vector
  val maxLen : int
  val array : int * 'a -> 'a array
  val fromList : 'a list -> 'a array
  val tabulate : int * (int -> 'a) -> 'a array
  val length : 'a array -> int
  val sub : 'a array * int -> 'a
  val update : 'a array * int * 'a -> unit
  val vector : 'a array -> 'a vector
  val copy : {di:int, dst:'a array, src:'a array} -> unit
  val copyVec : {di:int, dst:'a array, src:'a vector} -> unit
  val appi : (int * 'a -> unit) -> 'a array -> unit
  val app : ('a -> unit) -> 'a array -> unit
  val modifyi : (int * 'a -> 'a) -> 'a array -> unit
  val modify : ('a -> 'a) -> 'a array -> unit
  val foldli : (int * 'a * 'b -> 'b) -> 'b -> 'a array -> 'b
  val foldri : (int * 'a * 'b -> 'b) -> 'b -> 'a array -> 'b
  val foldl : ('a * 'b -> 'b) -> 'b -> 'a array -> 'b
  val foldr : ('a * 'b -> 'b) -> 'b -> 'a array -> 'b
  val findi : (int * 'a -> bool) -> 'a array -> (int * 'a) option
  val find : ('a -> bool) -> 'a array -> 'a option
  val exists : ('a -> bool) -> 'a array -> bool
  val all : ('a -> bool) -> 'a array -> bool
  val collate : ('a * 'a -> order) -> 'a array * 'a array -> order
  val toList : 'a array -> 'a list
  val fromVector : 'a vector -> 'a array
  val toVector : 'a array -> 'a vector
val myintarray = [|100,42,42,42,42,42,42,42,42,42|] : int array
val mystrarray =
  [|"The answer is","The answer is","The answer is","The answer is",
   "The answer is","The answer is","The answer is","The answer is",
   "The answer is","The answer is"|] : string array
#+end_example

2. Additional utility functions and libraries are available as part
   of the SML of NL library. This is also available in mlton

** Unique value creation

1. Not Globally unique but unique in a particular run of the computation.


#+BEGIN_SRC sml

datatype order = LESS | GREATER | EQUAL

signature UNIQUE = sig

   type uniq

   val new : unit -> uniq

end

(* opaque signature *)
structure Unique :> UNIQUE = struct
  type uniq = int

  val uniqRef = ref 0

  (* wrong implementation *)
  fun new () = 10 (* fixme *)



end

val x = Unique.new ()
(* val y = [x, 42] *)



(* Problem: Nothing gurantees that all the unique
   values that I created in the program is via Unique.new ()
 *)


#+END_SRC

#+RESULTS:
: datatype order = EQUAL | GREATER | LESS
: signature UNIQUE =
:   sig
:     type uniq
:     avl new : unit -> uniq
:   end
: structure Unique : UNIQUE
: val x = - : Unique.uniq


2. Atom implementation

A representation for variables in programs.

#+BEGIN_SRC sml

signature ATOM = sig

   type atom

   val atom  : string -> atom
   val toString : atom -> string

   val compare : atom * atom -> order

....
end

structure Atom :> ATOM = struct

   type atom = int

   val atomRef = ref 0

   (*
   1. A referencce to map from int (atom) -> string

   2. A reference to a map from string -> int (atom)

   *)

   type toStringMap = string IntRedBlackMap.map
   type atomMap     = atom StringRedBlackMap.map

   val toStringRef : ref toStringMap = ref IntRedBlackMap.empty
   val atomMap     : ref atomMap     = StringRedBlackMap.empty

   fun atom str = let val toStrMP = !toStringRef
                   (* loopkup the given str in toStrMP
                      1. It is already there in which case return the associated atom (int)
                      2. otherwise increment atomRef and assign str to this new value
                    *)

   fun toString atm = lookup
end

#+BEGIN_SRC sml

(* int -> int *)

val const ( _ : int) = 0
val incr x = x + 1


(* Given a function of type 'a -> 'a, 'a -> 'b -> 'a , int -> b -> int  *)

fun foo x = x

#+END_SRC

Benign side effect :: A side effectful computation that can be treated
like a pure computation The atom and toString functions of the Atom
structure look pure outside the structure although internally they
have side effect.


#+END_SRC
* DONE Lambda calculus and computability                          :CLASSROOM:

** Local declarations (non-recursive let).


#+BEGIN_SRC sml
fun foo x = let val y = x + 1
            in y * x
            end

#+END_SRC

#+RESULTS:
: val foo = fn : int -> int

#+BEGIN_EXAMPLE

let v = e1
in e2

===

(λ v . e2) e1

(fn v => e2) e1

....x = .. (let x = e1 in e2) ....

   The x in the expression e1 will be coming from
the outside scope

#+END_EXAMPLE

One can think of ~let v = e1 in e2~ as a syntactic sugar for
~(λ v . e2) e1~


Non-recursive let explanation
#+BEGIN_EXAMPLE

      <-----------
                 |
        (let x = x + 1 in .... x ...)
             ^                 |
             +-----------------+


  (let x = 5
     in
       let x = x + 1  (* this binds x to 5 + 1 = 6 *)
          in x        (* x here is 6 *)
       end
     end
  )

  This entire expression evaluates to 6
#+END_EXAMPLE

In OCAML there is ~let~ and ~let rec~

** Recursion and Fixpoints

1. Every lambda calculus expression is a function

2. A fixpoint of a function say F is a value X such that F X = X.

*** Fixpoint theorem

1. Every *closed* lambda calculus function has a fixed point.


2. This fixpoint is effectively computable. There is a λ-calculus
   combinator ~Y~ such that for all closed lambda calculus expressions
   ~F~, ~YF~ is the fixed point of ~F~.

- Closed expressions :: Expressions without free variables
- Combinator  :: Another name for closed expression.

- Consequence of part 1 :: In terms of recursive definition this means
     that any recursive definition of a function can be achieved
     through computing the appropriate fixpoint.

- Consequence of part 2 :: Think of a compiler that takes your
     favourite programming language and converts it into
     λ-calculus. The second part allows you to write such a compiler
     for recursive definitions.

     For a recursive equation like fact this means that you can write
     it as.

#+BEGIN_EXAMPLE
let F = fn f => fn n => if n <= 0 then 1 else n * f (n - 1)
   in let fact = Y F
          in  ....
          end
   end
#+END_EXAMPLE

Only closed expressions have meanings completely determined. In general,
the value associated with an expression depends on its free variable.

~λ x . x~ :: This is the identity function.

~x~ :: The value is determined only when the value of x is determined.


*** Fix points gives recursion.

1. letrec f x = ... f y .... in e

   F = λ f x . .....f y .....

#+BEGIN_SRC sml

fun fact n = if n <= 0 then 1
             else n * fact (n - 1)

val y = fact 4 (* this is just to check fact *)


fun F f n = if n <= 0 then 1 else n * f (n - 1) (* notice there is no recursion *)

val G = fn f => fn n => if n <= 0 then 1 else n * f (n - 1)

#+END_SRC

#+RESULTS:
: val fact = fn : int -> int
: val y = 24 : int
: val F = fn : (int -> int) -> int -> int
: val G = fn : (int -> int) -> int -> int
                  A               A

#+BEGIN_EXAMPLE

fact = λ n . if n <= 0 then 1 else n * fact (n - 1)

We want fact to be a solution of the above recursion


F = λ f . λ n . if n <= 0 then 1 else n * f (n - 1)

g is the fixpoint of F

This means g satisfies the recursion

   g = F g

g = (λ f . λ n . if n <= 0 then 1 else n * f (n - 1)) g
  = λ n . if n <= 0 then 1 else n * g (n - 1)

Solution of the recursion
 f = M (f)
 is the fixpoint of  λ f . M (f)

#+END_EXAMPLE

1. What is the fixpoint of ~F~ if it exists ?

   The fixpoint of ~F~ is that ~X~ such that ~F X = X~.

   ~F X is fn n => if n <= 0 then 1 else n * X (n - 1)~

   ~F X = X~ means ~X = F X~ which means

   ~X = fn n => if n <= 0 then 1 else n * X (n - 1)~
   ~fact = fn n => if n <= 0 then 1 else n * fact (n - 1)~

2. The recursive function definition ~fact~ is nothing but the fixpoint of
   The function ~F = fn f => fn n => if n <= 0 then 1 else n * f (n - 1)~


3. Notice that ~F~ is not recursive but its fixpoint is the solution to
   the recursive equation for ~fact~.

*** Proof of fixpoint theorem

1. We want the fixpoint of F

    θ = fn x => F (x x)

    θ θ = (fn x => F ( x x)) θ

        ⇒(β-reduces) F (x x) [ x := θ ]

        = F (θ θ)

        This proves that θθ is the fixpoint of F because θθ = F (θθ)

    Note that F (x x) is different from ((F x) x)
2. The Y-combinator for computing fixpoint.

   Given F the fixpoint of F is (λ x . F ( x x ))(λ x . F (x x))

   Y is that function that maps F to (λ x . F ( x x ))(λ x . F (x x))

   Y = λ f .  (λ x . f (x x)) (λ x . f (x x))

   Y is the desired fixpoint combinator.

   Y F = (λ f .  (λ x . f (x x)) (λ x . f (x x))) F
       = (λ x . F (x x))(λ x . F (x x))
       which is the fix point of F by part 1.

*** Some fun fixpoint combinators.

#+BEGIN_SRC

θ = λ p . λ k . k (p p k)

P = θ θ

P F = θ θ F  =  (λ p . λ k . k (p p k)) θ F
             => (λ k . k (θ θ k)) F
             => F ( θ θ F)

θθ is a fixpoint combinator like Y.

θ =   λ n λ e λ r λ a λ  j .  j(neeraj)

P = θ θ θ θ θ

Claim is P is a fixpoint combinator.

P F = (λ n λ e λ r λ a λ  j .  j(neeraj)) θ θ θ θ F
    = (λ j . j (θ θ θ θ θ j)) F)

    = F (θ θ θ θ θ F)
    = F (P F)



#+END_SRC

* DONE Typed Lambda calculus                                      :CLASSROOM:
<2020-10-05 Mon>

** So far.

So far we have seen untyped lambda calculus. It is a Turing complete
programming language, i.e. any computation task can be solved using
lambda calculus. We can have a semi-decision algorithm for any
recursively enumerable language of {0,1}*.

Lambda calculus is a full-fledged programming language.

1. There are no bultin constants or types like integers.

Adding types to lambda calculus. Constants and functions (builtin).


** λ-calculii and types.

The thing of interest for us is judgements ~e : τ~ where ~e~ is a term
and ~τ~ is a type.


*** The world of types

1. Start with some basic types, for eg. ~nat~, ~bool~, ....

2. The function type ~τ₁ -> τ₂~

#+BEGIN_EXAMPLE

<type> := one of the basic types like nat bool etc
       | <type>₁ -> <type>₂          (* function type or arrow type *)


#+END_EXAMPLE

*** The world of terms.

(λ x : τ . e)

These judgements e : τ, what do they mean ?  2 : nat means that we
cannot "apply" 2 to anything.

Suppose you know that e : nat then

1. You can use e in any context that expects a nat.  e.g (e + 1) is
   allowed

2. You cannot use e where a nat is not allowed e.g. (e x) is not
   allowed.


The fact that e : nat rules out the use of e in certain context in a
λ-calculus term. Or in other words, consider any context where e occurs
in a λ-calculus term M, we can decide whether e is used like a nat or not.


*** Types as a specification to values.

The judgement e : nat should be seen as a /specification/ of the
"program" e.

1. A specification or spec is a property that the program satisfies.
   For example, for scheduling /fairness/, i.e  any process that is
   ready should be scheduled to run in a finite amount of time.

2. A spec for a sorting program is that it "sorts the list".

   - Non-contracting :: If x is the input to the sort program then
        length of x = length of (sort x).


*** There are two problems associated with types.

- Type checking :: Given a term ~e~ and a type ~τ~, can we derive the
                   judgement ~e : τ~.

- Type inference :: Given a term ~e~ compute a type ~τ~ such that ~e :
                    τ~ is derivable.


We have to give the type rules for the calculus that we are talking
about.




** Simply typed lambda calculus.


1. Types: Basic types + arrow types.

   τ := nat
     |  τ₁ -> τ₂


When we want to derive the judgement ~e : τ~, we need a typing
context, i.e.  We need as assumption a set of judgements ~Γ = { x :
τ₁, x₂ , τ₂ .... }~ for all free variables of ~e~.


Say I want to arrive at this conclusion.
~x : nat~

I cannot /assert/ ~x : nat~ unless I /know/ that ~x : nat~.

The judgement ~x : nat~ is only valid under the assumption ~x : nat~.

x : nat ⊢ x : nat


~Γ ⊢ e : τ ~  is to be read as the judgement ~e : τ~ is valid in the context ~Γ~

A typing context Γ is just a list of type assumptions on variables.

Γ = { x₁ : τ₁, ... xₙ : τₙ }.


** Rules of typing.

*** Syntax consists of

- Variables :: x, y , z ..
- Function application :: e₁ e₂ ,i.e. The function  e₁ applied on e₂
- function abstraction :: (λ x:τ . e)

*** Typing rules

**** Types

#+BEGIN_EXAMPLE

τ := nat
  | τ₁ -> τ₂ |

#+END_EXAMPLE


There should be a rule for all the three term construction rules.

- A General Rule looks like ::

  #+BEGIN_EXAMPLE

   Pre-cond1
   Pre-cond2
   ...
   Pre-condn

   =========================

   conclusion.


#+END_EXAMPLE

- VAR :: The variable rule.

  #+BEGIN_EXAMPLE


   ===================
    {x : τ} ⊢ x : τ

  #+END_EXAMPLE

  Under no pre-condition, we can derive the judgement x : τ under the context x : τ

- APP ::

  #+BEGIN_EXAMPLE

    Γ ⊢ e₁ : τ₁ -> τ₂
    Γ ⊢ e₂ : τ₁

    =======================

    Γ ⊢ e₁ e₂ : τ₂


  #+END_EXAMPLE

  - Pre-condition ::
     ~e₁ : τ₁ -> τ₂~ under the context ~Γ~ and
     ~e₂ : τ₁~ under the cotnext ~Γ~

  - Conclusion ::
    ~e₁ e₂ : τ₂~ under the context ~Γ~.
- ABS ::

  #+BEGIN_EXAMPLE

   Γ ∪ { x : τ₁ } ⊢  e : τ₂

   ==============================

   Γ ⊢ (λ x : τ₁ . e) : τ₁ -> τ₂

  #+END_EXAMPLE

  - Pre-condition :: With the assumption ~x : τ₁~ suppose I derive ~e : τ₂~ then
  - Conclusion  :: (λ x : τ₁ . e) : τ₁ -> τ₂
* DONE Typed Lambda calculus Contd                                :CLASSROOM:

** Typing rule review

We want to have a complete set of rules for forming the judgement ~e : τ~ for each
λ-calculus term ~e~

~Γ~ is a set of type assumptions (i.e. judgements of the kind ~x : τ~). Conclusions are always
of the form ~Γ ⊢ e : τ~ (I can derive the judgement ~e : τ~ from the set of type assumptions ~Γ~.

Every Rule looks like
#+BEGIN_EXAMPLE
Pre-conditions
===============
Conclusion

#+END_EXAMPLE


- VAR :: The variable rule for the judgement ~x : τ~.

  #+BEGIN_EXAMPLE


   ===================
    Γ ∪ {x : τ} ⊢ x : τ

  #+END_EXAMPLE
- APP :: The application rule for judgement ~e₁ e₂ : τ~

  #+BEGIN_EXAMPLE

    Γ ⊢ e₁ : τ₁ -> τ₂
    Γ ⊢ e₂ : τ₁

    =======================

    Γ ⊢ e₁ e₂ : τ₂


  #+END_EXAMPLE
- ABS :: The abstraction rule for judgement ~fun (x : τ₁) => e  : τ₁ -> τ₂

  #+BEGIN_EXAMPLE

   (Γ ∪ { x : τ₁ }) ⊢  e : τ₂

   ==============================

   Γ ⊢ (λ x : τ₁ . e) : τ₁ -> τ₂
  #+END_EXAMPLE

- Weakening ::

#+BEGIN_EXAMPLE

   Γ ⊢ e : τ
   ==========
   Γ ∪ {x : τ₁} ⊢ e : τ


#+END_EXAMPLE



~Γ ⊢ e : τ~, I can derive the type judgement ~e: τ~
given the assumption set ~Γ~

We are now talking about the ABS rule.

- Judgement that I want to derive ::
     ~(fun (x : τ₁) => e)  : τ₁ -> τ₂~

When can the above function i.e. (fun (x : τ₁) => e)
have the type ~τ₁ -> τ₂~ ?
- Answer :: only when I have the pre-conditon that
            e : τ₂ under the assumption x : τ₁

** Some proofs of judgements.

A proof of a judgement should be a list of judgements of the kind Γ ⊢ e : τ₁. where

J₁ (R₁) ,J₂ (R₂),...,Jₙ (Rₙ)

(1) Rᵢ's are one of the instatiations of the the rule VAR, ABS, APP.

(2) every judgement Jᵢ follows from a set of judgements { Jₗ : l < i}  using one of the VAR,ABS or
    APP rule.


Jₙ is the judgement  (fun x : int => fun y : bool => x) : int -> bool -> int


1. ~x : int            ⊢ x : int~
                                   (VAR)
2. ~x : int, y : bool  ⊢ x : int~
                                  (VAR | WEAKENING (1) )

3. ~x : int            ⊢ (fun y : bool => x)  : bool -> int~
                                  (ABS (2))

4. ~⊢ fun (x : int) => fun (y : bool) => x) : int -> bool -> int~
                                  (ABS (3))
Γ ∪ x : τ₁ ⊢ e : τ₂
====================
Γ | (fun x : τ₁ => e) : τ₁ -> τ₂

#+END_EXAMPLE

In the case of 3 what is Γ ?
- Answer :: { x : int }
What is e is ?
- Answer :: (fun y : bool => x)

What is τ₁ ?
- Answer :: bool

What is τ₂ ?
- Answer :: int

What is x ?
- Answer :: y


** Exercise

Prove the judgement ~⊢ (fn x : nat => fn y : bool => x) 3 true : nat~

You can also use the additional Rules

#+BEGIN_EXAMPLE

===========           (TRUE-rule)
 true : bool

===========          (FALSE-rule)
 false : bool

===========  (NAT-n rule for each natural number n)
  n : nat

#+END_EXAMPLE

#+BEGIN_EXAMPLE
λ x y . x y  is same as λ x . λ y . x y

fn x y => x y   is same as fn x => (fn y => x y)

#+END_EXAMPLE
* DONE Polymorphism and Hindley-Milner types                      :CLASSROOM:

** Types

*** Monotypes

Mono-types or monomorphic types.

#+BEGIN_EXAMPLE
τ = nat
  | α        type variable
  | τ₁ -> τ₂
#+END_EXAMPLE

- Explanations ::
  These types are called mono-morphic types.

  "fixed but arbitrary"

   Should distinguish the mono-type ~α~ from the
   poly type ~∀ α . α~.


#+BEGIN_EXAMPLE

nat -> ∀ α. α -> ∀ β. β -> nat

#+END_EXAMPLE


*** Poly types.

- Type schemes ::
    #+BEGIN_EXAMPLE

  σ = τ  (where τ is a mono type)
    | ∀ α . σ₁  where (α is some type variable and σ₁ is some type scheme.
  #+END_EXAMPLE

  In general a type scheme ~σ = ∀α₁∀α₂ ...∀αₙ τ~.  In our type
                  checking/inference algorithms we only allow for
                  these types.

  - Note : If σ₁ and σ₂ are type schemes, then it is not always
    the case that ~σ₁ → σ₂~ is a type scheme.
    If σ₁ = ∀α. α → α and σ₂ = ∀ β. β  ~σ₁ → σ₂~ is  (∀α. α → α) →  (∀ β. β)
    Which is clearly not a type scheme.

- Background ::

-  We cannot have arbitrary polytypes as that would make the type
  checking problem undecidable.  For Hindley-Milner
  types we assume all the type variables are quantified
  in the /outer most layer/.

| Example types                    | Hindley-Milner or not |
|----------------------------------+-----------------------|
| nat                              | yes                   |
|----------------------------------+-----------------------|
| α -> β                           | yes                   |
|----------------------------------+-----------------------|
| σ₁ = ∀ α . (α -> β)              | yes                   |
|----------------------------------+-----------------------|
| σ₂ = ∀ α . (α -> (∀ β . α -> β)) | no                    |


- Without the Hindley-Milner restriction, type checking itself becomes undecidable.
  System-F is the lambda calculus where the ∀ quatification is allowed at any layer.
  For System-F type checking is undecidable. That is the reason we restrict to only
  that fragment of System-F where all the quatification is in the outer most layer.

#+BEGIN_SRC sml

val map : ('a -> 'b) -> 'a list -> 'b list

#+END_SRC

~map : ∀ α ∀ β . (α → β) → α list → β list~

** Type inference rules

We have the same rules as that of simply types lambda calculus (i.e. VAR, APP and ABS) for
all monotypes.

*** Some additional rules.

- Specialisation ::

  Γ ⊢ e : σ
  σ₁ ≤ σ
  ========================
  Γ ⊢ e : σ₁




~map : ∀ α ∀ β . (α → β) → α list → β list~

Let τ₁ and τ₂ be any monotype then clearly I can have the judgement
map : (τ₁ → τ₂) → τ₁ list → τ₂ list

map : ∀ γ₁ ∀ γ₂ ∀ γ₃ . (γ₁ -> (γ₃ * γ₂)) -> γ₁ list -> (γ₃ * γ₂) list

By substituting α with γ₁  and β with γ₃ * γ₂

map : (γ₁ -> (γ₃ * γ₂)) -> γ₁ list -> (γ₃ * γ₂) list


.
.
.
⊢ map : ∀ α ∀ β . (α → β) → α list → β list~


.
.
.
⊢ map : (γ₁ -> (γ₃ * γ₂)) -> γ₁ list -> (γ₃ * γ₂) list
⊢ map : ∀ γ₁ ∀ γ₂ ∀ γ₃ . (γ₁ -> (γ₃ * γ₂)) -> γ₁ list -> (γ₃ * γ₂) list


Let σ = ∀ α₁ ... ∀ αₙ . τ

1. τ [α₁ := τ₁]...[αₙ := τₙ] = τ' ≤ σ

2. ∀ β₁ ... ∀ βₘ . τ [α₁ := τ₁]...[αₙ := τₙ] = σ' ≤ σ

    Provided βᵢ's are not free variables of σ


~map : ∀ α ∀ β . (α → β) → α list → β list~

(map (fn x => [x]))

In the above context map should have type

∀ α . (α -> α list ) -> α list -> α list list


- Generalisation ::

Γ ⊢ e : σ
=============== (α ∉ FTV(Γ))
Γ ⊢ e : ∀ α . σ


Γ = { x₁ : σ₁, ..... , xₙ : σₙ }

FTV(Γ) = FV(σ₁) ∪ ... ∪ FV(σₙ)

If α ∉ FTV(Γ) then the judgement Γ ⊢ e : σ  can be proved for all α because it
is not part of the assumption Γ.

- This is a wrong proof as it violates the side condition
  α ∉ FTV(Γ)

1. {x : α} ⊢ x : α
2. {x : α} ⊢ x : ∀ α . α  (violation of side condition)
3. {x : α} ⊢ x : ∀ β . β


- THis proof is correct

1. {x : α} ⊢ x : α     (VAR rule)

2. ⊢ λ x . x : α → α   (ABS rule with 1)

         (No matter what is α we have a proof of the Judgement λ x . x : α → α)

3. ⊢ λ x . x : ∀ α . α → α (GEN on 2)


* DONE Type inference algorithm.                                  :CLASSROOM:


B = {nat , bool}

** Mono-types

#+BEGIN_EXAMPLE
τ = t            t ∈ B
  | τ₁ → τ₂      τ₁, τ₂ are monotypes
  | α            α is a type variable
#+END_EXAMPLE

** Type schemes

σ = ∀ α₁....∀αₙ . τ

#+BEGIN_EXAMPLE
σ = τ
  | ∀ α . σ'
#+END_EXAMPLE


** Typing rules


1. For monotypes it is the same as that of simply typed lambda calculus.

*** Bultins

#+BEGIN_EXAMPLE

================ for all n ∈ ℕ
 ⊢ n : nat

#+END_EXAMPLE


*** Var

=============
x : τ ⊢ x : τ


*** App

#+BEGIN_EXAMPLE


Γ ⊢ e₁ : τ₁ -> τ₂
Γ ⊢ e₂ : τ₁

===================

Γ ⊢ e₁ e₂ : τ₂


#+END_EXAMPLE


*** Abs

#+BEGIN_EXAMPLE

Γ , x : τ₁ ⊢ e : τ₂
====================
Γ         ⊢  λ x . e : τ₁ -> τ₂

#+END_EXAMPLE


** For type schemes

*** Specialisation

Γ ⊢ e : σ
σ'  ≤ σ
==========
Γ ⊢ e : σ'

*** Generalisation

Γ ⊢ e : σ
α ∉ FV (Γ)
================
Γ ⊢ e : ∀ α . σ


Notice that σ  ≤  ∀ α . σ


** Type inference problem

Given a set of typing assumption Γ and an expression e in the calculus
 find a type scheme σ such that Γ ⊢ e : σ.

1. We want σ to be the most general possible type. Suppose σ' is
   another type scheme such that Γ ⊢ e : σ' then we should have σ' ≤ σ

2. If e is ill-typed under Γ, i.e there is no type σ such that Γ ⊢ e :
   σ then the algorithm should flag a typing error

#+BEGIN_EXAMPLE

e = n     n ∈ ℕ
  | true
  | false
  | plus
  | and
  | x    x is a variable
  | e₁ e₂
  | λ x . e

#+END_EXAMPLE

** Algorithm W

- Main idea :: e is recursively defined so do a recursion on the
               structure of e.

- Second idea :: The algorithm will compute a monotype τ such that
                 the closure σ of τ under Γ is the most general possible type scheme such
                 that Γ ⊢ e : σ

- Third idea :: The algorithm should also keep track of all substitutions that it performed
                during the APP recursion step.

** Algorithm W

- Input  :: Γ a type assumption, A substitution Sᵢₙ and an expression e
- Output :: a monotype τ and a substitution Sₒᵤₜ such that Sᵢₙ (Γ) ⊢ e : τ'
  such that Closure (τ', Sᵢₙ(Γ)) is the most general such type and
  τ' = Sₒᵤₜ(τ)



Closure(τ, Γ) = ∀ α₁....∀ αₙ . τ  where {α₁,....,αₙ} = FV(τ) ∖ FV(Γ)

Γ = x : α

e = λ y . x  : β -> α = τ

THen Closure(τ,Γ) = ∀ β . β -> α


#+BEGIN_EXAMPLE
.
.
.
Γ ⊢ e : τ
Γ ⊢ e : ∀ α₁ ....∀ αₙ . τ  (GEN n-times)



#+END_EXAMPLE



*** For builtins

 e = n for some n ∈ ℕ

 Then we know that the most general type scheme
 σ such that Γ ⊢ e : σ is just ~nat~.

*** Variable

Let us say e is some variable x

Let σ be the type assigned to x in the type assumption Γ

Γ = Γ' ∪ {x : σ}


Let σ = ∀ α₁,...∀ αₙ . τ

Compute fresh type variables β₁,...,βₙ.  return the type e : τ' = τ
[α₁ := β₁] .... [αₙ := βₙ] i.e replace all type variables αᵢ with the
corresponding fresh variable βᵢ

e is x
the returned type is Sᵢₙ(τ')

Closure of τ' is  ∀ β₁...̱∀ βₙ . τ'  = σ







WHat ever output we give say τ, we need the fact that

Γ ⊢ e : τ

*** App case

e = e₁ e₂

1. Recursively compute the type τᵢ associated with eᵢ under the type assumption Γ
   If any of this fails then failure. Otherwise

   W(Γ, Sᵢₙ, e₁) = (S₁, τ₁) and
   W(Γ, S₁, e₂)  = (S₂, τ₂)

   However I should have the property that  e₁ : α -> β and e₂ : α

   Get hold of fresh type variables α and β.

   Unify (α -> β ≡ τ₁ , α ≡ τ₂)  Will get a substitution S.

   Return (S, S(β))



*** Abs case

e = λ x . e' . If at all e is well typed then it should be something like
e : τ₁ → τ₂


Γ' = Γ ∪ {x : α} for some fresh type variable α.

W(Γ', e', Sᵢₙ) = (Sₒᵤₜ, τ)

Return of W(Γ, e, Sᵢₙ) = ( Sₒᵤₜ, τ' -> τ)  where τ' is Sₒᵤₜ (α)

#+BEGIN_EXAMPLE

e = λ x . plus x 1

#+END_EXAMPLE

Γ = ∅

{x : α}  Compute type of plus x 1 Abs setp

{x : α} : Compute type of (plus x)  Compute the type of (1)  App

{x : α} : Compute type of plus, Compute type of x (App rule)


plus : nat -> nat -> nat

x : alpha

Unify (β -> γ) ≡ nat -> (nat -> nat), β ≡ α

α ≡ nat



#+BEGIN_EXAMPLE

Γ , x : τ₁ ⊢ e : τ₂
====================
Γ         ⊢  λ x . e : τ₁ -> τ₂

#+END_EXAMPLE







#+BEGIN_EXAMPLE


Γ ⊢ e₁ : τ₁ -> τ₂
Γ ⊢ e₂ : τ₁

===================

Γ ⊢ e₁ e₂ : τ₂


#+END_EXAMPLE


#+BEGIN_SRC sml
val e = map (fn x => x + 1) : int list -> int list
#+END_SRC

map : ('a -> 'b) -> 'a list -> 'b list

'a = int
 'b = int

* TODO Verification, Logic, Constructive Mathmatics

1. Programming

2. Correct programs. Want to ensure that programs are "correct"

3. Specification. We say what the program needs to do rather than telling how to do.

4. Correctness of programs means that programs satisfy the specification that it give for this.

We want to sort a list of numbers. bubble sort, quick sort, merge sort (how to sort question)

Give me a specification for a sorting algorithm.

1. The sorted list should be of the same length as the input.

2. ∀ x ∈ l, x ∈ sort l

3. The output of the algorithm should be monotonically increasing.

The identity function on list is satisfies the above spec but is not clearly a sorting algorithm.
Reverse function.


Formal specification

1. There should be a language to express the properties
2. The proofs should be machine checked.


The type system is a formal system.

Consider a language that does not enforce correctness of types at compile time.
A lot of bugs escapes to the runtime where it is much more costly.

1. Write test cases to catch bugs. TDD test-driven development.


You want a mechanism for proving program correct.

If you have a strong type system then clearly bugs like "hello" + 42 will be caught.


You can think of the typing rules as proofs in a logic which is automatically checked.

** Typing rules.

*** Builtins

#+BEGIN_EXAMPLE

===============  for all n ∈ ℕ
⊢ n : nat

#+END_EXAMPLE


#+BEGIN_EXAMPLE

===============
⊢ true : bool

#+END_EXAMPLE

... (the rest is exercise)

#+BEGIN_EXAMPLE

===========================
⊢ plus : nat -> nat -> nat


#+END_EXAMPLE

#+BEGIN_EXAMPLE

====================
⊢ and : bool -> bool -> bool

#+END_EXAMPLE

*** Variables

To determine the type of x, a variable, we need to assume its type.

#+BEGIN_EXAMPLE

===================
 {x : τ}  ⊢ x : τ


====================   A rule in logic
  A ⊢ A

#+END_EXAMPLE

*** Application

#+BEGIN_EXAMPLE

Γ ⊢ e₁ : τ₁ → τ₂
Γ ⊢ e₂ : τ₁
====================
Γ ⊢ e₁ e₂ : τ₂


Γ ⊢ A → B
Γ ⊢ A
===================
Γ ⊢ B


(modus-ponens)
#+END_EXAMPLE

1. Make sure that ~e₁ : τ₁ -> τ₂~

2. Make sure that ~e₂ : τ₁~

*** Abstraction

#+BEGIN_EXAMPLE

Γ ∪ { x : τ₁ } ⊢ e : τ₂
========================================
Γ              ⊢ (λ x : τ₁ . e)  : τ₁ → τ₂

#+END_EXAMPLE


Γ ∪ {A } ⊢ B
================
Γ ⊢ A → B



Suppose I want to prove assuming Γ that  A => B. It is sufficient to prove B assuming A (and Γ)


λ x : τ₁ . e  is that function that takes x : τ₁ and gives out e : τ₂



A ∨ B


#+BEGIN_EXAMPLE

Γ ⊢ A
========

Γ ⊢ A ∨ B


Γ ⊢ B
========

Γ ⊢ A ∨ B


Γ ⊢ A -> C
Γ ⊢ B ->  C
=================
Γ ⊢  A ∨ B ->  C


#+END_EXAMPLE


#+BEGIN_SRC sml

(* or of logic *)
datatype ('a, 'b) Sum = left of 'a
                      | right of 'b


fun either f g (left a) = f a
  | either f g (right b)= g b

(* like the and of logic *)
type ('a,'b) prod = 'a * 'b


#+END_SRC

#+RESULTS:
: datatype ('a,'b) Sum = left of 'a | right of 'b
: val either = fn : ('a -> 'b) -> ('c -> 'b) -> ('a,'c) Sum -> 'b


#+BEGIN_EXAMPLE

Γ ⊢ e : 'a
===========================
Γ ⊢ left e : ('a , 'b) Sum


Γ ⊢ f : 'a -> 'c
Γ ⊢ g : 'b -> 'c
==================
Γ ⊢ either f g : ('a,'b) Sum -> 'c

#+END_EXAMPLE


** Curry-Howard Correspondence


Think of types as Logical Statements and terms as purported proofs of
statements. Type checking rules have a correspondence with that of
proof checking.


Suppose I have a language which very rich type system, Then The type
checking algorithm can double up as a proof checking algorithm.

In such languages.

1. You capture a statement as a type T in the language

2. You prove the statement T by giving an element x : T.


Such a language can be used as

1. A programming language

2. As a proof assistant : means a program that checks whether the given proof
   is correct

3. Think of it as both.

You can write programs and prove programs correct in the same language.

Coq, Isabelle, HOL etc






*** Structural rules.
**** Weakening

#+BEGIN_EXAMPLE

Γ               ⊢ e : τ
====================
Γ ∪ { x : τ₁ }  ⊢ e : τ
#+END_EXAMPLE

**** Contraction

#+BEGIN_EXAMPLE

Γ ∪ { x : τ₁ } ∪  { x : τ₁ }  ⊢ e : τ
===================================
Γ ∪ { x : τ₁ }  ⊢ e : τ

#+END_EXAMPLE


* TODO Rust: An approach to safe, low-level programming

** Main ideas.

- C/C++ like low level language.

C is like portable assembly. The advantages of this is that you can
code pretty "close to the metal" the call it. The programming model
and the processor model is not very different.

C/C++ is extreamly unsafe. Addresses and integers can be confused. No
array bound checking. Dangling pointers. Use without allocation etc.

Some of these bugs do not manifest even at runtime.

If there is a segmentation fault (which essentially is due to
accessing locations that are invalid) there is no way to even
reproduce these bugs often.

In SML for example computation happens as a set of reductions/rewritings.

- But safety like High level language.

  Catch the above bugs.


Rust "borrows" many interesting ideas from languages like SML (by
default things are immutable) It also codifies some best practices
from C++ programming (RIAA) making violation of this compile time
bugs.

- Targets low level programming.

  Things like operating systems, device drivers, programming language
  runtimes etc.


Idea for language/library designers

If possible push potential bugs from runtime to compile time.

** Hello world program

#+BEGIN_SRC rust
// C++ like line comments
/*

/* Nested C like comments.

*/

*/
fn main ()
{
  println!("hello world");  // semi-colon is a separator not a terminator
  println!("goodbye world")
}
#+END_SRC

** Mutability and type inference

#+BEGIN_SRC rust

fn main ()
{

let x = 10; // x is immutable
x = 5 // is an error because x is immutable

// Infinite loop

}
#+END_SRC

1. Rust is careful about mutabililty and like SML things are immutable
2. Rust can infer types.

** Things that need resource

Values in your language can be simple straight forward values like int
bool etc but they might also be ID's for resources.

For example an operating system lock is a resource.

1. Values can be "names" to resources. Like for example a socket in
   an OS can be represented by a integer but a socket really is much
   more than an integers.

2. Such values might need exclusive access

3. Values might point to memory resources. Pointers ultimately are integers but
   it is not the integral value of the pointer that is of interest to us.

   1. What is being pointed
   2. How much memory is available at this point etc

Rust gives you a way to manage values that have such resources tied to it.
It makes sure that "exclusivity" is maintained

Ownership model of Rust.

** Resource that I will discuss is Boxed values.

There are two kinds of values

-  Unboxed :: The value is the entire description of the object, examples are like boolean,
   integer constant (within the word size of the machine).

   Copying of unboxed value is easy

-  Boxed :: The value is not the entire story. E.g pointers or multi-precision Integers (i.e
       which cannot fit in the machine word)

   Copying has to distinguish between shallow copy vs
   deep copy.

   - Shallow copying of the list type in C is just copying the list pointer

   - A deep copying of a list is

#+BEGIN_SRC C

typedef struct Node
           { int data;
             struct Node *next;
           } Node;

typedef Node *List

List x y z ptr qtr;

x = SomeFunctionToCreateList()

y = x ; // This is a shallow copy as you are only copying the pointer

// Shallow copy leeds to dangling pointers.

// This is a deep copying
for(ptr = x; ptr != NULL; ptr = ptr -> next)
{
  qtr = malloc(...)
  qtr -> datum = ptr -> datum;
....
 }

// You might forget to free. Memory leaks


}

#+END_SRC


In C++ one would want to overload the assignment operators.

1. Should assignment operator do deep copy or shallow copy.

In C++ since memory is managed manually the decision can have consequences.

x = y ; in C++ might either be doing a shallow copy or a deep copy

If it is a shallow copy, I should not free y if it is no more used
If it is a deep copy, it makes sense to free y if it is no more used

f(x)

** Rusts ownership model


There are two types of values. One that follows
the Copy semantics and other that follows the move semantics

An assignment of the kind ~x = y~ is treated differently
depending on whether the type of x (and y) follows
Copy semantics or move semantics.

- Copy semantics :: x = y should seen as make a
copy of the value in y and store it in x

- Move semantics :: x = y should be seen as transfer
the value (and its ownership) to x

Example

~u32~ is an example of a type (unsigned 32-bit integer)
that follows the Copy semantics

~Box<u32>~ is an example of a type that follows Move
semantics.


#+BEGIN_SRC rust

fn main ()
{
   let x : Box<u32> = Box::new(10);
   let v = 10

   bar(v);

   foo(x)


/ foo(x); // let y = x

// let u = x;
// blah(u)

// let z = x

   ...
   ...

}

// This is in a library

fn foo (u : Box<u32>)
{
  blah(u)
}
#+END_SRC





** Freeing of resources in rust

In rust an object/resource is freed when it has no owner or
equivalently as soon as the owner dies (when the last owner goes out
of scope). (No manual freeing is required)


This is correct because Rust ensures at compile time that
there is only one owner for any value that satisfies the
Move semantics.


** Ownership and Borrowing.

The two rules of Ownership and Borrow.

1. There is at any point of time only one owner for any value
2. There is atmost one mutable borrow of a resource


- Owner ship transfers from x to y when we perform ~y = x~ ;
- When the last owner of the value dies then resource is freed.


Assignment to a reference variable should be seen as the reference
variable borrowing the value/resource

#+BEGIN_EXAMPLE rust

let xbox = Box:new(10);
let ybox = xbox; // ownership is with ybox.
let ubox = Box::new(42);

let zbox = &ybox;  // this is only a borrow the type of zbox is &Box<u32>

let z = 5;
let mut u = 5;

/*
let <var> = ...
let mut <var> = ...

*/
// z = 10;  // is an error because z is immutable
          // so z cannot be changed.

u = 10 ; // This is okey because u is mutable

//  zbox = &ubox // This is an error as zbox
                 // is an immutable variable.
                 //


/*
  (zbox is a immutable variable) that has the immutable reference to ybox
*/


let (mut vbox) = &ybox; //

/*

(vbox is a mutable variable) of ...

So vbox can be reassigned

*/
vbox = &ubox ; // This is allowed
              // The borrow from ybox is
              // no more there but now vbox
              // has a borrow from ubox
// foo(&ubox) ;; fn foo(u : &Box<u32>)
// u = &ubox ... body of the function
// let vbox : &mut Box<u32> = &...

// The reference allows you to update the
// contents



#+END_EXAMPLE




#+BEGIN_SRC rust

let x = value

/*
&x is immutable ref to x and &mut x is error

*/

let mut y = value
let mut y' = ....
/*
  &y is an immutable reference to y and &mut y is a mutable reference to y

Through &y you cannot change y but through &mut y you can change y.

*/

let xr = &mut y

*xr = value'  // This is allowed

xr = &mut y' // This is not allowd as xr is changed

/* What happens here ? */

#+END_SRC

<

1. If ~let x = value means &x has to be immutable
      let mut x = value means &x is an immutable reference and &mut x is the mutable reference to x


1. A ~&mut x~ (mutable reference of x) of a immutable variable x is error
2. A ~*x = value~ is error unless x is of type ~&mut T~

Rules of borrowing

1. You can have any number of immutable reference

2. You can have atmost one mutable reference. If there is a mutable reference then
   no other borrowing is allowed (whether mutable or not). There can be only one
   "route" to update the contents of a variable

#+BEGIN_SRC rust

let mut x = value
{

  let y = &mut x
  let z = &x // is not allowd because the borrow y is a mutable borrow and thus
                disallows any other borrow (mutable or not)

} // y's life time is over so now you can have as many borrow as you want
let z = &x  // is not allowed
let z1 = &x//

#+END_SRC

The reason is

With a immutable reference you can only "read" the contents not modify. It is okey to
have any number of them
